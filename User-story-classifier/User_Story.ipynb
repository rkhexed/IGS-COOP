{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "232c91d1-0b2a-4e7c-83b7-4547941a29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dfb33ab-8a0c-465f-bc5a-9c591b9562d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "data = pd.read_csv('userstory.csv', encoding='ISO-8859-1')\n",
    "cdata = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3206140-e845-422f-864f-7143e1b68a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: Index(['User Stories', 'Description',\n",
      "       'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]',\n",
      "       'Software Application [frontent, backend, mobile, desktop, cloud, gaming]',\n",
      "       'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cdata.columns = cdata.columns.str.strip()\n",
    "print(\"Column Names:\", cdata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd8fcb7-0023-46a8-a43d-633920b0ca77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        User Stories  \\\n",
      "0           Pass layoutCohort to mentioned P13N APIs   \n",
      "1  Optimisation and Fixes in BYA Service and Cons...   \n",
      "2     Make Who's Watching Screen Title Config Driven   \n",
      "3                    Making Profile Switching Easier   \n",
      "4                 Launch for Trending trays via A/B    \n",
      "\n",
      "                                         Description  \\\n",
      "0  *Context* : P13N services needs to make scylla...   \n",
      "1  # Add LD flags for state to language mapping f...   \n",
      "2  New config to be added on Launch Darkly - {{wh...   \n",
      "3  Helping Users Understand the concept of Multi ...   \n",
      "4  We are building Trending recommendations for o...   \n",
      "\n",
      "  Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]  \\\n",
      "0                                                NaN                         \n",
      "1                                                NaN                         \n",
      "2                                              Media                         \n",
      "3                                                NaN                         \n",
      "4                                                NaN                         \n",
      "\n",
      "  Software Application [frontent, backend, mobile, desktop, cloud, gaming]  \\\n",
      "0                                            backend                         \n",
      "1                                            backend                         \n",
      "2                                           frontend                         \n",
      "3                                           frontend                         \n",
      "4                                           frontend                         \n",
      "\n",
      "  Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]  \n",
      "0                                                NaN                                                                                                                     \n",
      "1                                                NaN                                                                                                                     \n",
      "2                                         Jio Cinema                                                                                                                     \n",
      "3                                                NaN                                                                                                                     \n",
      "4                                                NaN                                                                                                                     \n"
     ]
    }
   ],
   "source": [
    "# Drop rows where 'Domain', 'Software', and 'Platform' are all null\n",
    "cleaned_data = cdata.dropna(subset=['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]', 'Software Application [frontent, backend, mobile, desktop, cloud, gaming]', 'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'], how='all')\n",
    "\n",
    "# Check the cleaned data\n",
    "print(cleaned_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83989375-ce15-4cbf-a822-f13719404b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: Index(['User Stories', 'Description',\n",
      "       'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]',\n",
      "       'Software Application [frontent, backend, mobile, desktop, cloud, gaming]',\n",
      "       'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cleaned_data.columns = cleaned_data.columns.str.strip()\n",
    "print(\"Column Names:\", cleaned_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0325b78a-b807-458b-ac92-dd757b61460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows: 3410\n",
      "Number of rows after cleaning: 1469\n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows before cleaning\n",
    "initial_rows = data.shape[0]\n",
    "\n",
    "# Get the number of rows after cleaning\n",
    "cleaned_rows = cleaned_data.shape[0]\n",
    "\n",
    "# Print the number of rows\n",
    "print(f\"Initial number of rows: {initial_rows}\")\n",
    "print(f\"Number of rows after cleaning: {cleaned_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abea775b-f042-451c-9482-d7371bd7fc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User Stories                                                                                                                                                               0\n",
       "Description                                                                                                                                                              224\n",
       "Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]                                                                                                 252\n",
       "Software Application [frontent, backend, mobile, desktop, cloud, gaming]                                                                                                  55\n",
       "Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]    205\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5e1e34-0411-4944-a43d-3add46cca043",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = cleaned_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3b9e4f-4636-48a5-a519-cc375df3d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = cleaned_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d046713-0d4e-4349-86d2-728748af1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = cleaned_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8bdb275-73a1-4c3e-aed3-ed877e01291e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Media', 'Entertainment', 'Finance', 'Education',\n",
       "       'E-commerce'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae397bb-241c-4665-b52c-94fabaf8535d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['backend', 'frontend', 'cloud', 'mobile', nan, 'gaming'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80de1e36-485b-465f-8fac-58dccc408c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Jio Cinema', 'Web', 'Playback', 'Stage', 'Android', 'iOS',\n",
       "       'MixPanel', 'AndroidTV', 'CQC', 'Celia', 'All Platforms', 'Spike',\n",
       "       'All platforms', 'Playstore', 'Fanverse', 'CTV', 'ATV', 'Jio STB',\n",
       "       'JioTV', 'H5TV', 'Jio ChargeIT', 'SDK', 'AppleTV', 'VOOT'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb56ce67-a038-44fe-971c-6e8c14c9c03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]\n",
       "Media            978\n",
       "E-commerce        86\n",
       "Entertainment     52\n",
       "Finance           51\n",
       "Education         50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28007a37-90cc-4db2-9ed8-ea050487ace8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Software Application [frontent, backend, mobile, desktop, cloud, gaming]\n",
       "backend     528\n",
       "mobile      397\n",
       "frontend    339\n",
       "cloud       110\n",
       "gaming       40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c24ff64a-ba50-421e-a449-828c46e6e1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]\n",
       "Android          324\n",
       "iOS              225\n",
       "Web              178\n",
       "Jio Cinema       157\n",
       "VOOT             120\n",
       "Playback         102\n",
       "ATV               31\n",
       "Jio ChargeIT      27\n",
       "All platforms     24\n",
       "AndroidTV         20\n",
       "Fanverse          18\n",
       "Playstore          7\n",
       "Spike              6\n",
       "Celia              5\n",
       "Jio STB            4\n",
       "H5TV               3\n",
       "All Platforms      2\n",
       "JioTV              2\n",
       "CQC                2\n",
       "SDK                2\n",
       "AppleTV            2\n",
       "MixPanel           1\n",
       "CTV                1\n",
       "Stage              1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3980bad-8485-46e7-9a42-6fcad3041c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]\n",
      "Android          324\n",
      "iOS              225\n",
      "Web              178\n",
      "Jio Cinema       157\n",
      "VOOT             120\n",
      "Playback         102\n",
      "TV                63\n",
      "Jio ChargeIT      27\n",
      "All platforms     24\n",
      "Fanverse          18\n",
      "Playstore          7\n",
      "Spike              6\n",
      "Celia              5\n",
      "CQC                2\n",
      "All Platforms      2\n",
      "SDK                2\n",
      "Stage              1\n",
      "MixPanel           1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merging specific platforms into a new category 'TV'\n",
    "tv_platforms = ['AndroidTV', 'ATV', 'H5TV', 'Jio STB', 'JioTV', 'CTV', 'AppleTV']\n",
    "\n",
    "# Create a copy of y3\n",
    "y3_merged = y3.copy()\n",
    "\n",
    "# Replace the specified platforms with 'TV'\n",
    "y3_merged = y3_merged.replace(tv_platforms, 'TV')\n",
    "\n",
    "# Check the new value counts after merging\n",
    "platform_counts = y3_merged.value_counts()\n",
    "print(platform_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d95b87e-4682-44e8-b3bb-7ffe81094734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]\n",
      "Android          324\n",
      "iOS              225\n",
      "Web              178\n",
      "Jio Cinema       157\n",
      "VOOT             120\n",
      "Playback         102\n",
      "TV                63\n",
      "Jio ChargeIT      27\n",
      "All platforms     24\n",
      "Fanverse          18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the value counts of the merged platform data\n",
    "platform_counts = y3_merged.value_counts()\n",
    "\n",
    "# Identify platforms with counts of 10 or more\n",
    "platforms_to_keep = platform_counts[platform_counts >= 10].index.tolist()\n",
    "\n",
    "# Filter the y3_merged to keep only rows with the specified platforms\n",
    "y3_cleaned = y3_merged[y3_merged.isin(platforms_to_keep)]\n",
    "\n",
    "# Display the value counts of the cleaned data\n",
    "cleaned_platform_counts = y3_cleaned.value_counts()\n",
    "print(cleaned_platform_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "498cd196-a218-4794-96d1-36b3f13e7ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]\n",
       "Media            978\n",
       "E-commerce        86\n",
       "Entertainment     52\n",
       "Finance           51\n",
       "Education         50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "021d8578-59c2-41be-942d-04f0a3b1a8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Software Application [frontent, backend, mobile, desktop, cloud, gaming]\n",
       "backend     528\n",
       "mobile      397\n",
       "frontend    339\n",
       "cloud       110\n",
       "gaming       40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0d3f3eb-3263-43d7-8cdb-663690d1d99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]\n",
       "Android          324\n",
       "iOS              225\n",
       "Web              178\n",
       "Jio Cinema       157\n",
       "VOOT             120\n",
       "Playback         102\n",
       "TV                63\n",
       "Jio ChargeIT      27\n",
       "All platforms     24\n",
       "Fanverse          18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3_cleaned.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a55366b9-ce62-4017-999e-49ff50b9eef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names: Index(['User Stories', 'Description',\n",
      "       'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]',\n",
      "       'Software Application [frontent, backend, mobile, desktop, cloud, gaming]',\n",
      "       'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Column Names:\", cleaned_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f039a29-a246-4935-89c5-f0816c8ea90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                            User Stories  \\\n",
      "0              Pass layoutCohort to mentioned P13N APIs   \n",
      "1     Optimisation and Fixes in BYA Service and Cons...   \n",
      "2        Make Who's Watching Screen Title Config Driven   \n",
      "3                       Making Profile Switching Easier   \n",
      "4                    Launch for Trending trays via A/B    \n",
      "...                                                 ...   \n",
      "3405  As a user, I want to access character bios and...   \n",
      "3406  As a gamer, I want to create and share my own ...   \n",
      "3407  As a player, I want to receive feedback on my ...   \n",
      "3408  As a user, I want to have access to multilingu...   \n",
      "3409  As a gamer, I want to utilize augmented realit...   \n",
      "\n",
      "                                            Description  \\\n",
      "0     *Context* : P13N services needs to make scylla...   \n",
      "1     # Add LD flags for state to language mapping f...   \n",
      "2     New config to be added on Launch Darkly - {{wh...   \n",
      "3     Helping Users Understand the concept of Multi ...   \n",
      "4     We are building Trending recommendations for o...   \n",
      "...                                                 ...   \n",
      "3405  - Character bios should be easily navigable wi...   \n",
      "3406  - User-created content should be easily sharea...   \n",
      "3407  - Feedback should be provided through an analy...   \n",
      "3408  - Language options should be selectable within...   \n",
      "3409  - AR features should be optional and easily ac...   \n",
      "\n",
      "     Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]  \\\n",
      "0                                                   NaN                         \n",
      "1                                                   NaN                         \n",
      "2                                                 Media                         \n",
      "3                                                   NaN                         \n",
      "4                                                   NaN                         \n",
      "...                                                 ...                         \n",
      "3405                                      Entertainment                         \n",
      "3406                                      Entertainment                         \n",
      "3407                                      Entertainment                         \n",
      "3408                                      Entertainment                         \n",
      "3409                                      Entertainment                         \n",
      "\n",
      "     Software Application [frontent, backend, mobile, desktop, cloud, gaming]  \\\n",
      "0                                               backend                         \n",
      "1                                               backend                         \n",
      "2                                              frontend                         \n",
      "3                                              frontend                         \n",
      "4                                              frontend                         \n",
      "...                                                 ...                         \n",
      "3405                                             gaming                         \n",
      "3406                                             gaming                         \n",
      "3407                                             gaming                         \n",
      "3408                                             gaming                         \n",
      "3409                                             gaming                         \n",
      "\n",
      "     Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]  \n",
      "0                                                   NaN                                                                                                                     \n",
      "1                                                   NaN                                                                                                                     \n",
      "2                                            Jio Cinema                                                                                                                     \n",
      "3                                                   NaN                                                                                                                     \n",
      "4                                                   NaN                                                                                                                     \n",
      "...                                                 ...                                                                                                                     \n",
      "3405                                            Android                                                                                                                     \n",
      "3406                                                iOS                                                                                                                     \n",
      "3407                                            Android                                                                                                                     \n",
      "3408                                                iOS                                                                                                                     \n",
      "3409                                            Android                                                                                                                     \n",
      "\n",
      "[1469 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a880857a-39bf-44e1-844b-31c01229e827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/kmq275hx6xvflqnbfp5t2vk80000gp/T/ipykernel_98352/494726911.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_data['combined_text'] = cleaned_data['User Stories'] + \" \" + cleaned_data['Description']\n"
     ]
    }
   ],
   "source": [
    "# Combine story and description into one column for vectorization\n",
    "cleaned_data['combined_text'] = cleaned_data['User Stories'] + \" \" + cleaned_data['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "abaa2478-b67b-45e3-aa8d-bf74b4ed9b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.25.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8c62c34-b0a2-4763-a081-09a6b1adf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/kmq275hx6xvflqnbfp5t2vk80000gp/T/ipykernel_98352/6898585.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  cleaned_data['combined_text'].fillna('', inplace=True)\n",
      "/var/folders/c_/kmq275hx6xvflqnbfp5t2vk80000gp/T/ipykernel_98352/6898585.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_data['combined_text'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Fill missing values in the 'combined_text' column with an empty string\n",
    "cleaned_data['combined_text'].fillna('', inplace=True)\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # You can adjust the number of features\n",
    "\n",
    "# Fit and transform the combined text data\n",
    "X = tfidf.fit_transform(cleaned_data['combined_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2cd8f95e-6193-481c-843a-bb1b8a85b088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Fill missing values in the 'combined_text' column with an empty string\n",
    "cleaned_data['combined_text'].fillna('', inplace=True)\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "X = model.encode(cleaned_data['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a790c69d-5f61-446d-a495-639fb943cb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 (Domain) Encoded: [5 5 4 ... 2 2 2]\n",
      "y2 (Software) Encoded: [0 0 2 ... 3 3 3]\n",
      "y3_cleaned (Platform) Encoded: [4 8 5 ... 1 9 1]\n"
     ]
    }
   ],
   "source": [
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode y1 (Domain), y2 (Software), and y3_cleaned (Platform)\n",
    "y1_encoded = le.fit_transform(y1)\n",
    "y2_encoded = le.fit_transform(y2)\n",
    "y3_cleaned_encoded = le.fit_transform(y3_cleaned)\n",
    "\n",
    "print(f\"y1 (Domain) Encoded: {y1_encoded}\")\n",
    "print(f\"y2 (Software) Encoded: {y2_encoded}\")\n",
    "print(f\"y3_cleaned (Platform) Encoded: {y3_cleaned_encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "861c1077-f5ac-40be-ba46-ed570593116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (556, 5000), y1_train shape: (556,)\n",
      "X_train_y2 shape: (556, 5000), y2_train shape: (556,)\n",
      "X_train_y3 shape: (556, 5000), y3_train shape: (556,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # Adjust number of features as needed\n",
    "\n",
    "# Create the feature matrix X from combined text\n",
    "# Make sure you're using cleaned_data and not any previous filtered data\n",
    "X = tfidf.fit_transform(cleaned_data['combined_text']).toarray()\n",
    "\n",
    "# Create the target DataFrame\n",
    "target_data = pd.DataFrame({\n",
    "    'y1': y1,\n",
    "    'y2': y2,\n",
    "    'y3_cleaned': y3_cleaned\n",
    "})\n",
    "\n",
    "# Concatenate features and targets\n",
    "data_with_targets = pd.DataFrame(X)\n",
    "data_with_targets['y1'] = target_data['y1']\n",
    "data_with_targets['y2'] = target_data['y2']\n",
    "data_with_targets['y3_cleaned'] = target_data['y3_cleaned']\n",
    "\n",
    "# Drop rows with any missing values in the target columns\n",
    "data_with_targets = data_with_targets.dropna()\n",
    "\n",
    "#Recreate the feature matrix X and the target variables after alignment\n",
    "X_cleaned = data_with_targets.iloc[:, :-3].values  # All columns except last three (targets)\n",
    "y1_encoded = data_with_targets['y1'].values\n",
    "y2_encoded = data_with_targets['y2'].values\n",
    "y3_cleaned_encoded = data_with_targets['y3_cleaned'].values\n",
    "\n",
    "# Split for Domain (y1)\n",
    "X_train, X_test, y1_train, y1_test = train_test_split(X_cleaned, y1_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split for Software (y2)\n",
    "X_train_y2, X_test_y2, y2_train, y2_test = train_test_split(X_cleaned, y2_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split for Platform (y3_cleaned)\n",
    "X_train_y3, X_test_y3, y3_train, y3_test = train_test_split(X_cleaned, y3_cleaned_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display shapes to verify everything is correct\n",
    "print(f\"X_train shape: {X_train.shape}, y1_train shape: {y1_train.shape}\")\n",
    "print(f\"X_train_y2 shape: {X_train_y2.shape}, y2_train shape: {y2_train.shape}\")\n",
    "print(f\"X_train_y3 shape: {X_train_y3.shape}, y3_train shape: {y3_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "376a0b39-ae48-4d9b-b9a0-2034f7a3f52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating models for y1 (Domain) ---\n",
      "Random Forest Accuracy: 0.9429\n",
      "Gradient Boosting Accuracy: 0.9357\n",
      "Support Vector Classifier Accuracy: 0.9429\n",
      "Logistic Regression Accuracy: 0.9429\n",
      "\n",
      "--- Evaluating models for y2 (Software) ---\n",
      "Random Forest Accuracy: 0.3929\n",
      "Gradient Boosting Accuracy: 0.3357\n",
      "Support Vector Classifier Accuracy: 0.3571\n",
      "Logistic Regression Accuracy: 0.3643\n",
      "\n",
      "--- Evaluating models for y3 (Platform) ---\n",
      "Random Forest Accuracy: 0.2714\n",
      "Gradient Boosting Accuracy: 0.2643\n",
      "Support Vector Classifier Accuracy: 0.2714\n",
      "Logistic Regression Accuracy: 0.2714\n"
     ]
    }
   ],
   "source": [
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Support Vector Classifier': SVC(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results[model_name] = accuracy\n",
    "        print(f'{model_name} Accuracy: {accuracy:.4f}')\n",
    "    return results\n",
    "\n",
    "# Train and evaluate for each target variable\n",
    "print(\"\\n--- Evaluating models for y1 (Domain) ---\")\n",
    "results_y1 = evaluate_models(X_train, X_test, y1_train, y1_test)\n",
    "\n",
    "print(\"\\n--- Evaluating models for y2 (Software) ---\")\n",
    "results_y2 = evaluate_models(X_train_y2, X_test_y2, y2_train, y2_test)\n",
    "\n",
    "print(\"\\n--- Evaluating models for y3 (Platform) ---\")\n",
    "results_y3 = evaluate_models(X_train_y3, X_test_y3, y3_train, y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572b430-1bbb-42cd-9634-c57499cd8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Print the sizes of the datasets\n",
    "print(\"Sizes of the datasets:\")\n",
    "print(f\"X_train shape: {X_train.shape}, y1_train shape: {y1_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y1_test shape: {y1_test.shape}\")\n",
    "\n",
    "print(f\"\\nX_train_y2 shape: {X_train_y2.shape}, y2_train shape: {y2_train.shape}\")\n",
    "print(f\"X_test_y2 shape: {X_test_y2.shape}, y2_test shape: {y2_test.shape}\")\n",
    "\n",
    "print(f\"\\nX_train_y3 shape: {X_train_y3.shape}, y3_train shape: {y3_train.shape}\")\n",
    "print(f\"X_test_y3 shape: {X_test_y3.shape}, y3_test shape: {y3_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff35ae-2fa3-46cb-a860-9ef6035b950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_data.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1de75363-0cde-4ca7-b874-40210be3760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in /opt/anaconda3/lib/python3.12/site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in /opt/anaconda3/lib/python3.12/site-packages (from tf-keras) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.18,>=2.17->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7326aa4f-28eb-427e-a35c-99cf1701c7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (4.66.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57a45565-ad78-4c01-a674-b731b683a0af",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([            nan,             nan,         'Media',             nan,\\n                   nan,         'Media',             nan,         'Media',\\n               'Media',             nan,\\n       ...\\n       'Entertainment', 'Entertainment', 'Entertainment', 'Entertainment',\\n       'Entertainment', 'Entertainment', 'Entertainment', 'Entertainment',\\n       'Entertainment', 'Entertainment'],\\n      dtype='object', length=1469)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_class\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Set the column to classify and define the candidate labels\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m candidate_labels_y1 \u001b[38;5;241m=\u001b[39m cleaned_data[y1]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Unique values of y1\u001b[39;00m\n\u001b[1;32m     17\u001b[0m candidate_labels_y2 \u001b[38;5;241m=\u001b[39m cleaned_data[y2]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Unique values of y2\u001b[39;00m\n\u001b[1;32m     18\u001b[0m candidate_labels_y3 \u001b[38;5;241m=\u001b[39m cleaned_data[y3_cleaned]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Unique values of y3_cleaned\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([            nan,             nan,         'Media',             nan,\\n                   nan,         'Media',             nan,         'Media',\\n               'Media',             nan,\\n       ...\\n       'Entertainment', 'Entertainment', 'Entertainment', 'Entertainment',\\n       'Entertainment', 'Entertainment', 'Entertainment', 'Entertainment',\\n       'Entertainment', 'Entertainment'],\\n      dtype='object', length=1469)] are in the [columns]\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2dc7a1f0-a93b-4069-98d1-46a43669142b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([            nan,             nan,         'Media',             nan,\\n                   nan,         'Media',             nan,         'Media',\\n               'Media',             nan,\\n       ...\\n       'Entertainment', 'Entertainment', 'Entertainment', 'Entertainment',\\n       'Entertainment', 'Entertainment', 'Entertainment', 'Entertainment',\\n       'Entertainment', 'Entertainment'],\\n      dtype='object', length=1469)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_class\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Ensure y1, y2, and y3_cleaned are the actual column names, not variables holding data\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m candidate_labels_y1 \u001b[38;5;241m=\u001b[39m cleaned_data[y1]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Extract unique values from y1 column in DataFrame\u001b[39;00m\n\u001b[1;32m     17\u001b[0m candidate_labels_y2 \u001b[38;5;241m=\u001b[39m cleaned_data[y2]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Extract unique values from y2 column in DataFrame\u001b[39;00m\n\u001b[1;32m     18\u001b[0m candidate_labels_y3 \u001b[38;5;241m=\u001b[39m cleaned_data[y3_cleaned]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Extract unique values from y3_cleaned column in DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([            nan,             nan,         'Media',             nan,\\n                   nan,         'Media',             nan,         'Media',\\n               'Media',             nan,\\n       ...\\n       'Entertainment', 'Entertainment', 'Entertainment', 'Entertainment',\\n       'Entertainment', 'Entertainment', 'Entertainment', 'Entertainment',\\n       'Entertainment', 'Entertainment'],\\n      dtype='object', length=1469)] are in the [columns]\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bfe4041d-200e-4e61-b924-4c0bfab3cf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]\n",
       "streaming services    468\n",
       "Android               324\n",
       "iOS                   225\n",
       "Web                   178\n",
       "Jio ChargeIT           27\n",
       "Fanverse               18\n",
       "Playstore               7\n",
       "Spike                   6\n",
       "Celia                   5\n",
       "CQC                     2\n",
       "SDK                     2\n",
       "Stage                   1\n",
       "MixPanel                1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming_values = [\"Jio Cinema\", \"VOOT\", \"Playback\", \"ATV\", \"All Platforms\", \"Android TV\", \"Jio STB\", \n",
    "                    \"H5TV\", \"JioTV\", \"AppleTV\", \"CTV\", \"All platforms\", \"AndroidTV\"]\n",
    "\n",
    "# Replace the specified values with 'streaming services'\n",
    "cleaned_data[platform_col] = cleaned_data[platform_col].replace(streaming_values, 'streaming services')\n",
    "cleaned_data[platform_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4e452060-4532-4b17-80d9-6ca2d14045cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]\n",
       "streaming services    468\n",
       "Android               324\n",
       "iOS                   225\n",
       "Web                   178\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = cleaned_data[cleaned_data[platform_col].map(cleaned_data[platform_col].value_counts()) >= 30]\n",
    "cleaned_data[platform_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ee3ea511-ed0a-4eec-83ca-3f3f9626e539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Software Application [frontent, backend, mobile, desktop, cloud, gaming]\n",
       "mobile      395\n",
       "backend     378\n",
       "frontend    242\n",
       "cloud       101\n",
       "gaming       38\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[software_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "06ed7735-bcf8-4039-8f8f-5105b09da958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]\n",
       "Media            897\n",
       "E-commerce        57\n",
       "Finance           44\n",
       "Entertainment     40\n",
       "Education         40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[domain_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13a0382d-afe6-40f8-9103-fe1aaac37ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying combined_text: 100%|██████████| 1469/1469 [1:47:01<00:00,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Domain: 0.5867937372362151\n",
      "Accuracy for Software: 0.4567733151803948\n",
      "Accuracy for Platform: 0.4874063989108237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Attempt 1 with all values greater than 10 and combined TV value\n",
    "\"\"\"\n",
    "Accuracy for Domain: 0.5867937372362151\n",
    "Accuracy for Software: 0.4567733151803948\n",
    "Accuracy for Platform: 0.4874063989108237\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\", device=0)\n",
    "\n",
    "def classify_input(query, candidate_labels, n=3):\n",
    "    sequence_to_classify = f\"The query is {query}\"\n",
    "    result = classifier(sequence_to_classify, candidate_labels)\n",
    "    result_class = result[\"labels\"][:n]\n",
    "    return result_class\n",
    "    \n",
    "domain_col = 'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'\n",
    "software_col = 'Software Application [frontent, backend, mobile, desktop, cloud, gaming]'\n",
    "platform_col = 'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'\n",
    "\n",
    "# Prepare the candidate labels (unique values in each target column)\n",
    "candidate_labels_y1 = cleaned_data[domain_col].dropna().unique().tolist()\n",
    "candidate_labels_y2 = cleaned_data[software_col].dropna().unique().tolist()\n",
    "candidate_labels_y3 = cleaned_data[platform_col].dropna().unique().tolist()\n",
    "\n",
    "# Create empty columns to store the classification results\n",
    "    cleaned_data['predicted_Domain'] = None\n",
    "    cleaned_data['predicted_Software'] = None\n",
    "    cleaned_data['predicted_Platform'] = None\n",
    "    \n",
    "    # Progress bar setup for classifying combined_text\n",
    "    for idx, row in tqdm(cleaned_data.iterrows(), total=len(cleaned_data), desc=\"Classifying combined_text\"):\n",
    "        query = row['combined_text']\n",
    "        \n",
    "        # Classify for each target (Domain, Software, Platform)\n",
    "    predicted_y1 = classify_input(query, candidate_labels_y1)\n",
    "    predicted_y2 = classify_input(query, candidate_labels_y2)\n",
    "    predicted_y3 = classify_input(query, candidate_labels_y3)\n",
    "\n",
    "    # Store the results back into the DataFrame\n",
    "    cleaned_data.at[idx, 'predicted_Domain'] = predicted_y1[0] if predicted_y1 else None\n",
    "    cleaned_data.at[idx, 'predicted_Software'] = predicted_y2[0] if predicted_y2 else None\n",
    "    cleaned_data.at[idx, 'predicted_Platform'] = predicted_y3[0] if predicted_y3 else None\n",
    "\n",
    "# Check accuracy for each target class by comparing with the original data\n",
    "accuracy_y1 = (cleaned_data['predicted_Domain'] == cleaned_data[domain_col]).mean()\n",
    "accuracy_y2 = (cleaned_data['predicted_Software'] == cleaned_data[software_col]).mean()\n",
    "accuracy_y3 = (cleaned_data['predicted_Platform'] == cleaned_data[platform_col]).mean()\n",
    "\n",
    "print(f\"Accuracy for Domain: {accuracy_y1}\")\n",
    "print(f\"Accuracy for Software: {accuracy_y2}\")\n",
    "print(f\"Accuracy for Platform: {accuracy_y3}\")\n",
    "\n",
    "# Save the classified data with predictions to a new CSV file\n",
    "cleaned_data.to_csv('classified_cleaned_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a51af167-0d11-4658-a633-592a34202ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying combined_text: 100%|██████████| 1195/1195 [1:00:59<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Domain: 0.6585774058577406\n",
      "Accuracy for Software: 0.46778242677824267\n",
      "Accuracy for Platform: 0.5774058577405857\n"
     ]
    }
   ],
   "source": [
    "#Attempt 2 with combining all TV and streaming platforms into Streaming services, along with dropping everything except for Web, iOS and Android.\n",
    "\"\"\"\n",
    "Accuracy for Domain: 0.6585774058577406\n",
    "Accuracy for Software: 0.46778242677824267\n",
    "Accuracy for Platform: 0.5774058577405857\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\", device=0)\n",
    "\n",
    "def classify_input(query, candidate_labels, n=3):\n",
    "    sequence_to_classify = f\"The query is {query}\"\n",
    "    result = classifier(sequence_to_classify, candidate_labels)\n",
    "    result_class = result[\"labels\"][:n]\n",
    "    return result_class\n",
    "    \n",
    "domain_col = 'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'\n",
    "software_col = 'Software Application [frontent, backend, mobile, desktop, cloud, gaming]'\n",
    "platform_col = 'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'\n",
    "\n",
    "# Prepare the candidate labels (unique values in each target column)\n",
    "candidate_labels_y1 = cleaned_data[domain_col].dropna().unique().tolist()\n",
    "candidate_labels_y2 = cleaned_data[software_col].dropna().unique().tolist()\n",
    "candidate_labels_y3 = cleaned_data[platform_col].dropna().unique().tolist()\n",
    "\n",
    "# Create empty columns to store the classification results\n",
    "cleaned_data['predicted_Domain'] = None\n",
    "cleaned_data['predicted_Software'] = None\n",
    "cleaned_data['predicted_Platform'] = None\n",
    "    \n",
    "# Progress bar setup for classifying combined_text\n",
    "for idx, row in tqdm(cleaned_data.iterrows(), total=len(cleaned_data), desc=\"Classifying combined_text\"):\n",
    "    query = row['combined_text']\n",
    "        \n",
    "    # Classify for each target (Domain, Software, Platform)\n",
    "    predicted_y1 = classify_input(query, candidate_labels_y1)\n",
    "    predicted_y2 = classify_input(query, candidate_labels_y2)\n",
    "    predicted_y3 = classify_input(query, candidate_labels_y3)\n",
    "\n",
    "    # Store the results back into the DataFrame\n",
    "    cleaned_data.at[idx, 'predicted_Domain'] = predicted_y1[0] if predicted_y1 else None\n",
    "    cleaned_data.at[idx, 'predicted_Software'] = predicted_y2[0] if predicted_y2 else None\n",
    "    cleaned_data.at[idx, 'predicted_Platform'] = predicted_y3[0] if predicted_y3 else None\n",
    "\n",
    "# Check accuracy for each target class by comparing with the original data\n",
    "accuracy_y1 = (cleaned_data['predicted_Domain'] == cleaned_data[domain_col]).mean()\n",
    "accuracy_y2 = (cleaned_data['predicted_Software'] == cleaned_data[software_col]).mean()\n",
    "accuracy_y3 = (cleaned_data['predicted_Platform'] == cleaned_data[platform_col]).mean()\n",
    "\n",
    "print(f\"Accuracy for Domain: {accuracy_y1}\")\n",
    "print(f\"Accuracy for Software: {accuracy_y2}\")\n",
    "print(f\"Accuracy for Platform: {accuracy_y3}\")\n",
    "\n",
    "# Save the classified data with predictions to a new CSV file\n",
    "cleaned_data.to_csv('classified_cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e577a7a4-6c07-447c-9dd5-5582c5d8c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Classifying combined_text:   1%|        | 2/156 [02:29<3:12:14, 74.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m predictions_y1 \u001b[38;5;241m=\u001b[39m classify_input(batch, candidate_labels_y1)\n\u001b[1;32m     44\u001b[0m predictions_y2 \u001b[38;5;241m=\u001b[39m classify_input(batch, candidate_labels_y2)\n\u001b[0;32m---> 45\u001b[0m predictions_y3 \u001b[38;5;241m=\u001b[39m classify_input(batch, candidate_labels_y3)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Store the predictions back into the DataFrame\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch)):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Safeguard against potential missing predictions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 16\u001b[0m, in \u001b[0;36mclassify_input\u001b[0;34m(batch, candidate_labels)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_input\u001b[39m(batch, candidate_labels):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Use the pipeline to classify the entire batch\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     results \u001b[38;5;241m=\u001b[39m classifier(batch, candidate_labels)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/zero_shot_classification.py:206\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(sequences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1238\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1235\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1236\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1237\u001b[0m     )\n\u001b[0;32m-> 1238\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1164\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1163\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1164\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1165\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/zero_shot_classification.py:229\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    228\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n\u001b[1;32m    231\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: candidate_label,\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sequence,\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutputs,\n\u001b[1;32m    236\u001b[0m }\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1297\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1297\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeberta(\n\u001b[1;32m   1298\u001b[0m     input_ids,\n\u001b[1;32m   1299\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1300\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1301\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1302\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1303\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1304\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1305\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1306\u001b[0m )\n\u001b[1;32m   1308\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1309\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1063\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1055\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1056\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1057\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1061\u001b[0m )\n\u001b[0;32m-> 1063\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1064\u001b[0m     embedding_output,\n\u001b[1;32m   1065\u001b[0m     attention_mask,\n\u001b[1;32m   1066\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1067\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1068\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1069\u001b[0m )\n\u001b[1;32m   1070\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:507\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    497\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    498\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    499\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m         output_attentions,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    508\u001b[0m         next_kv,\n\u001b[1;32m    509\u001b[0m         attention_mask,\n\u001b[1;32m    510\u001b[0m         query_states\u001b[38;5;241m=\u001b[39mquery_states,\n\u001b[1;32m    511\u001b[0m         relative_pos\u001b[38;5;241m=\u001b[39mrelative_pos,\n\u001b[1;32m    512\u001b[0m         rel_embeddings\u001b[38;5;241m=\u001b[39mrel_embeddings,\n\u001b[1;32m    513\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    514\u001b[0m     )\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    517\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:355\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    348\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m ):\n\u001b[0;32m--> 355\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    356\u001b[0m         hidden_states,\n\u001b[1;32m    357\u001b[0m         attention_mask,\n\u001b[1;32m    358\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    359\u001b[0m         query_states\u001b[38;5;241m=\u001b[39mquery_states,\n\u001b[1;32m    360\u001b[0m         relative_pos\u001b[38;5;241m=\u001b[39mrelative_pos,\n\u001b[1;32m    361\u001b[0m         rel_embeddings\u001b[38;5;241m=\u001b[39mrel_embeddings,\n\u001b[1;32m    362\u001b[0m     )\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    364\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:286\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    285\u001b[0m ):\n\u001b[0;32m--> 286\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    287\u001b[0m         hidden_states,\n\u001b[1;32m    288\u001b[0m         attention_mask,\n\u001b[1;32m    289\u001b[0m         output_attentions,\n\u001b[1;32m    290\u001b[0m         query_states\u001b[38;5;241m=\u001b[39mquery_states,\n\u001b[1;32m    291\u001b[0m         relative_pos\u001b[38;5;241m=\u001b[39mrelative_pos,\n\u001b[1;32m    292\u001b[0m         rel_embeddings\u001b[38;5;241m=\u001b[39mrel_embeddings,\n\u001b[1;32m    293\u001b[0m     )\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    295\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:726\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    721\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m attention_scores\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, attention_scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), attention_scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    723\u001b[0m )\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# bsz x height x length x dimension\u001b[39;00m\n\u001b[0;32m--> 726\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m XSoftmax\u001b[38;5;241m.\u001b[39mapply(attention_scores, attention_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    727\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n\u001b[1;32m    728\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(\n\u001b[1;32m    729\u001b[0m     attention_probs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attention_probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), attention_probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), value_layer\n\u001b[1;32m    730\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:106\u001b[0m, in \u001b[0;36mXSoftmax.forward\u001b[0;34m(ctx, input, mask, dim)\u001b[0m\n\u001b[1;32m    103\u001b[0m rmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m(mask\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool))\n\u001b[1;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmasked_fill(rmask, torch\u001b[38;5;241m.\u001b[39mtensor(torch\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin))\n\u001b[0;32m--> 106\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(output, ctx\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m    107\u001b[0m output\u001b[38;5;241m.\u001b[39mmasked_fill_(rmask, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    108\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(output)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "# Load cleaned data\n",
    "cleaned_data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Drop rows where 'combined_text' is NaN\n",
    "cleaned_data.dropna(subset=['combined_text'], inplace=True)\n",
    "\n",
    "# Initialize the zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\", device=0)\n",
    "\n",
    "def classify_input(batch, candidate_labels):\n",
    "    # Use the pipeline to classify the entire batch\n",
    "    results = classifier(batch, candidate_labels)\n",
    "    return results\n",
    "\n",
    "# Define columns for classification\n",
    "domain_col = 'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'\n",
    "software_col = 'Software Application [frontent, backend, mobile, desktop, cloud, gaming]'\n",
    "platform_col = 'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'\n",
    "\n",
    "# Prepare the candidate labels (unique values in each target column)\n",
    "candidate_labels_y1 = cleaned_data[domain_col].dropna().unique().tolist()\n",
    "candidate_labels_y2 = cleaned_data[software_col].dropna().unique().tolist()\n",
    "candidate_labels_y3 = cleaned_data[platform_col].dropna().unique().tolist()\n",
    "\n",
    "# Create empty columns to store the classification results\n",
    "cleaned_data['predicted_Domain'] = None\n",
    "cleaned_data['predicted_Software'] = None\n",
    "cleaned_data['predicted_Platform'] = None\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 8\n",
    "\n",
    "# Progress bar setup for classifying combined_text\n",
    "for start_idx in tqdm(range(0, len(cleaned_data), batch_size), desc=\"Batch Classifying combined_text\"):\n",
    "    end_idx = min(start_idx + batch_size, len(cleaned_data))\n",
    "    batch = cleaned_data['combined_text'].iloc[start_idx:end_idx].tolist()\n",
    "    \n",
    "    # Classify the batch for each target (Domain, Software, Platform)\n",
    "    predictions_y1 = classify_input(batch, candidate_labels_y1)\n",
    "    predictions_y2 = classify_input(batch, candidate_labels_y2)\n",
    "    predictions_y3 = classify_input(batch, candidate_labels_y3)\n",
    "    \n",
    "    # Store the predictions back into the DataFrame\n",
    "    for i in range(len(batch)):\n",
    "        # Safeguard against potential missing predictions\n",
    "        cleaned_data.at[start_idx + i, 'predicted_Domain'] = predictions_y1[i][\"labels\"][0] if predictions_y1 else None\n",
    "        cleaned_data.at[start_idx + i, 'predicted_Software'] = predictions_y2[i][\"labels\"][0] if predictions_y2 else None\n",
    "        cleaned_data.at[start_idx + i, 'predicted_Platform'] = predictions_y3[i][\"labels\"][0] if predictions_y3 else None\n",
    "\n",
    "# Check accuracy for each target class by comparing with the original data\n",
    "accuracy_y1 = (cleaned_data['predicted_Domain'] == cleaned_data[domain_col]).mean()\n",
    "accuracy_y2 = (cleaned_data['predicted_Software'] == cleaned_data[software_col]).mean()\n",
    "accuracy_y3 = (cleaned_data['predicted_Platform'] == cleaned_data[platform_col]).mean()\n",
    "\n",
    "print(f\"Accuracy for Domain: {accuracy_y1}\")\n",
    "print(f\"Accuracy for Software: {accuracy_y2}\")\n",
    "print(f\"Accuracy for Platform: {accuracy_y3}\")\n",
    "\n",
    "# Save the classified data with predictions to a new CSV file\n",
    "cleaned_data.to_csv('classified_cleaned_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478505e1-c1ad-4c69-bdca-5948d0eadcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Load the cleaned data\n",
    "cleaned_data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Initialize the classifier pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\", device=0)\n",
    "\n",
    "# Define the function for classifying a single row\n",
    "def classify_row(row):\n",
    "    query = row['combined_text']\n",
    "    # Classify for each target (Domain, Software, Platform)\n",
    "    predicted_y1 = classify_input(query, candidate_labels_y1)\n",
    "    predicted_y2 = classify_input(query, candidate_labels_y2)\n",
    "    predicted_y3 = classify_input(query, candidate_labels_y3)\n",
    "    \n",
    "    return {\n",
    "        'predicted_Domain': predicted_y1[0] if predicted_y1 else None,\n",
    "        'predicted_Software': predicted_y2[0] if predicted_y2 else None,\n",
    "        'predicted_Platform': predicted_y3[0] if predicted_y3 else None\n",
    "    }\n",
    "\n",
    "def classify_input(query, candidate_labels, n=3):\n",
    "    sequence_to_classify = f\"The query is {query}\"\n",
    "    result = classifier(sequence_to_classify, candidate_labels)\n",
    "    result_class = result[\"labels\"][:n]\n",
    "    return result_class\n",
    "\n",
    "# Define the columns to classify\n",
    "domain_col = 'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'\n",
    "software_col = 'Software Application [frontent, backend, mobile, desktop, cloud, gaming]'\n",
    "platform_col = 'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'\n",
    "\n",
    "# Prepare the candidate labels (unique values in each target column)\n",
    "candidate_labels_y1 = cleaned_data[domain_col].dropna().unique().tolist()\n",
    "candidate_labels_y2 = cleaned_data[software_col].dropna().unique().tolist()\n",
    "candidate_labels_y3 = cleaned_data[platform_col].dropna().unique().tolist()\n",
    "\n",
    "# Create a list to hold the rows of predictions\n",
    "predictions = []\n",
    "\n",
    "# Use multiprocessing to classify the data\n",
    "if __name__ == '__main__':\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap(classify_row, [row for _, row in cleaned_data.iterrows()]), total=len(cleaned_data), desc=\"Classifying combined_text\"))\n",
    "    \n",
    "    # Populate the predictions back into the DataFrame\n",
    "    for result in results:\n",
    "        predictions.append(result)\n",
    "\n",
    "    # Expand predictions into the DataFrame\n",
    "    cleaned_data['predicted_Domain'] = [result['predicted_Domain'] for result in predictions]\n",
    "    cleaned_data['predicted_Software'] = [result['predicted_Software'] for result in predictions]\n",
    "    cleaned_data['predicted_Platform'] = [result['predicted_Platform'] for result in predictions]\n",
    "\n",
    "# Check accuracy for each target class by comparing with the original data\n",
    "accuracy_y1 = (cleaned_data['predicted_Domain'] == cleaned_data[domain_col]).mean()\n",
    "accuracy_y2 = (cleaned_data['predicted_Software'] == cleaned_data[software_col]).mean()\n",
    "accuracy_y3 = (cleaned_data['predicted_Platform'] == cleaned_data[platform_col]).mean()\n",
    "\n",
    "print(f\"Accuracy for Domain: {accuracy_y1}\")\n",
    "print(f\"Accuracy for Software: {accuracy_y2}\")\n",
    "print(f\"Accuracy for Platform: {accuracy_y3}\")\n",
    "\n",
    "# Save the classified data with predictions to a new CSV file\n",
    "cleaned_data.to_csv('classified_cleaned_data.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4ff9fc57-9cc0-48ff-9a28-e083ef8d2370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts in the new DataFrame:\n",
      "Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]\n",
      "Media            50\n",
      "E-commerce       50\n",
      "Entertainment    50\n",
      "Finance          50\n",
      "Education        50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned_data DataFrame\n",
    "cleaned_data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Column name for Domain\n",
    "domain_col = 'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'\n",
    "\n",
    "# List of the five categories\n",
    "categories = ['Media', 'E-commerce', 'Entertainment', 'Finance', 'Education']\n",
    "\n",
    "# Create an empty DataFrame to store the 250 rows\n",
    "sampled_data = pd.DataFrame()\n",
    "\n",
    "# For each category, sample 50 rows and append them to the new DataFrame\n",
    "for category in categories:\n",
    "    category_sample = cleaned_data[cleaned_data[domain_col] == category].sample(n=50, random_state=42)\n",
    "    sampled_data = pd.concat([sampled_data, category_sample])\n",
    "\n",
    "# Reset the index of the new DataFrame\n",
    "sampled_data = sampled_data.reset_index(drop=True)\n",
    "\n",
    "# Print the value counts of the Domain column in the new DataFrame\n",
    "print(\"Value counts in the new DataFrame:\")\n",
    "print(sampled_data[domain_col].value_counts())\n",
    "\n",
    "# Save the sampled data to a new CSV file if needed\n",
    "sampled_data.to_csv('sampled_cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "32a68637-f532-46ec-b22c-06ff8a2802fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]\n",
       "Android               60\n",
       "Web                   50\n",
       "iOS                   48\n",
       "streaming services    39\n",
       "Jio ChargeIT          12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'\n",
    "].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c1d872f4-f07b-4ccd-9fa5-b49787c6efa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Software Application [frontent, backend, mobile, desktop, cloud, gaming]\n",
       "cloud       82\n",
       "backend     69\n",
       "gaming      38\n",
       "frontend    33\n",
       "mobile      26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data[software_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bdab5603-4125-479a-b94b-583c44107749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]\n",
      "Android               60\n",
      "Web                   50\n",
      "iOS                   48\n",
      "streaming services    39\n",
      "Jio ChargeIT          12\n",
      "Fanverse               9\n",
      "Playstore              5\n",
      "Celia                  5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'cleaned_data' is your DataFrame and 'Platform' is the column with platform values\n",
    "streaming_values = [\"Jio Cinema\", \"VOOT\", \"Playback\", \"ATV\", \"All Platforms\", \"AndroidTV\", \"Jio STB\", \n",
    "                    \"H5TV\", \"JioTV\", \"AppleTV\", \"CTV\", \"All platforms\"]\n",
    "\n",
    "# Replace the specified values with 'streaming services'\n",
    "sampled_data[platform_col] = sampled_data[platform_col].replace(streaming_values, 'streaming services')\n",
    "\n",
    "# Verify the changes\n",
    "print(sampled_data[platform_col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e639db4b-d8db-4c74-8837-be7da3b28380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the sampled_data: 250\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows in the sampled_data DataFrame\n",
    "num_rows = sampled_data.shape[0]\n",
    "print(f\"Number of rows in the sampled_data: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "32b5a976-41ba-4998-bf79-7ff141040138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying combined_text: 100%|██████████████| 250/250 [12:25<00:00,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Domain: 0.656\n",
      "Accuracy for Software: 0.472\n",
      "Accuracy for Platform: 0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Attempt 3 With total of 250 rows with 50 from each of the Domains\n",
    "\"\"\"\n",
    "Accuracy for Domain: 0.656\n",
    "Accuracy for Software: 0.472\n",
    "Accuracy for Platform: 0.596\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\", device=0)\n",
    "\n",
    "def classify_input(query, candidate_labels, n=3):\n",
    "    sequence_to_classify = f\"The query is {query}\"\n",
    "    result = classifier(sequence_to_classify, candidate_labels)\n",
    "    result_class = result[\"labels\"][:n]\n",
    "    return result_class\n",
    "    \n",
    "domain_col = 'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'\n",
    "software_col = 'Software Application [frontent, backend, mobile, desktop, cloud, gaming]'\n",
    "platform_col = 'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'\n",
    "\n",
    "# Prepare the candidate labels (unique values in each target column)\n",
    "candidate_labels_y1 = sampled_data[domain_col].dropna().unique().tolist()\n",
    "candidate_labels_y2 = sampled_data[software_col].dropna().unique().tolist()\n",
    "candidate_labels_y3 = sampled_data[platform_col].dropna().unique().tolist()\n",
    "\n",
    "# Create empty columns to store the classification results\n",
    "sampled_data['predicted_Domain'] = None\n",
    "sampled_data['predicted_Software'] = None\n",
    "sampled_data['predicted_Platform'] = None\n",
    "    \n",
    "# Progress bar setup for classifying combined_text\n",
    "for idx, row in tqdm(sampled_data.iterrows(), total=len(sampled_data), desc=\"Classifying combined_text\"):\n",
    "    query = row['combined_text']\n",
    "        \n",
    "    # Classify for each target (Domain, Software, Platform)\n",
    "    predicted_y1 = classify_input(query, candidate_labels_y1)\n",
    "    predicted_y2 = classify_input(query, candidate_labels_y2)\n",
    "    predicted_y3 = classify_input(query, candidate_labels_y3)\n",
    "\n",
    "    # Store the results back into the DataFrame\n",
    "    sampled_data.at[idx, 'predicted_Domain'] = predicted_y1[0] if predicted_y1 else None\n",
    "    sampled_data.at[idx, 'predicted_Software'] = predicted_y2[0] if predicted_y2 else None\n",
    "    sampled_data.at[idx, 'predicted_Platform'] = predicted_y3[0] if predicted_y3 else None\n",
    "\n",
    "# Check accuracy for each target class by comparing with the original data\n",
    "accuracy_y1 = (sampled_data['predicted_Domain'] == sampled_data[domain_col]).mean()\n",
    "accuracy_y2 = (sampled_data['predicted_Software'] == sampled_data[software_col]).mean()\n",
    "accuracy_y3 = (sampled_data['predicted_Platform'] == sampled_data[platform_col]).mean()\n",
    "\n",
    "print(f\"Accuracy for Domain: {accuracy_y1}\")\n",
    "print(f\"Accuracy for Software: {accuracy_y2}\")\n",
    "print(f\"Accuracy for Platform: {accuracy_y3}\")\n",
    "\n",
    "# Save the classified data with predictions to a new CSV file\n",
    "#d_data.to_csv('classified_cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1460167f-46bc-4869-8c0d-54fc17a68661",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data.to_csv('sampled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fa0be7bd-2885-4d27-8533-41760f6a4244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data.columns[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "92aaa0e8-617b-41ff-8775-7d6f225c9869",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Platform [AndroidTV, ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Platform [AndroidTV, ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m X_train_y2, X_test_y2, y_train_y2, y_test_y2 \u001b[38;5;241m=\u001b[39m train_test_split(X, sampled_data[software_col], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Split for Platform\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m X_train_y3, X_test_y3, y_train_y3, y_test_y3 \u001b[38;5;241m=\u001b[39m train_test_split(X, sampled_data[platform_col], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Platform [AndroidTV, ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocessing step: Fill NaN values in combined_text column with an empty string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('')\n",
    "\n",
    "# Step 1: Extract features using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf_vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Step 2: Split data for each target (Domain, Software, Platform)\n",
    "domain_col = 'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'\n",
    "software_col = 'Software Application [frontent, backend, mobile, desktop, cloud, gaming]'\n",
    "platform_col = 'Platform [AndroidTV, ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'\n",
    "\n",
    "# Split for Domain\n",
    "X_train_y1, X_test_y1, y_train_y1, y_test_y1 = train_test_split(X, sampled_data[domain_col], test_size=0.2, random_state=42)\n",
    "\n",
    "# Split for Software\n",
    "X_train_y2, X_test_y2, y_train_y2, y_test_y2 = train_test_split(X, sampled_data[software_col], test_size=0.2, random_state=42)\n",
    "\n",
    "# Split for Platform\n",
    "X_train_y3, X_test_y3, y_train_y3, y_test_y3 = train_test_split(X, sampled_data[platform_col], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5c742f36-a1de-429a-912f-a1915dca328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in sampled_data:\n",
      "['User Stories', 'Description', 'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]', 'Software Application [frontent, backend, mobile, desktop, cloud, gaming]', 'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]', 'combined_text']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get all columns in the sampled_data DataFrame\n",
    "columns = sampled_data.columns.tolist()\n",
    "\n",
    "# Print the list of columns\n",
    "print(\"Columns in sampled_data:\")\n",
    "print(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "538839cc-3ab1-4dbd-9e18-a666d9808bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Domain:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.33      0.86      0.48         7\n",
      "    Education       1.00      1.00      1.00         9\n",
      "Entertainment       1.00      0.70      0.82        10\n",
      "      Finance       1.00      0.64      0.78        11\n",
      "        Media       0.78      0.54      0.64        13\n",
      "\n",
      "     accuracy                           0.72        50\n",
      "    macro avg       0.82      0.75      0.74        50\n",
      " weighted avg       0.85      0.72      0.75        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Software:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.42      1.00      0.59        10\n",
      "       cloud       1.00      1.00      1.00        16\n",
      "    frontend       0.67      0.20      0.31        10\n",
      "      gaming       1.00      0.88      0.93         8\n",
      "      mobile       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.70        50\n",
      "   macro avg       0.62      0.61      0.57        50\n",
      "weighted avg       0.70      0.70      0.65        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Platform:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.00      0.00      0.00         2\n",
      "           Android       0.50      0.60      0.55        10\n",
      "             Celia       0.00      0.00      0.00         3\n",
      "          Fanverse       0.00      0.00      0.00         2\n",
      "      Jio ChargeIT       0.17      0.50      0.25         2\n",
      "               Web       0.91      0.83      0.87        12\n",
      "               iOS       0.71      0.42      0.53        12\n",
      "streaming services       0.33      0.57      0.42         7\n",
      "\n",
      "          accuracy                           0.52        50\n",
      "         macro avg       0.33      0.37      0.33        50\n",
      "      weighted avg       0.54      0.52      0.51        50\n",
      "\n",
      "Best parameters found for Domain: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "# Assuming `sampled_data` is your DataFrame containing the required columns.\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model and hyperparameter grid\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1_macro')\n",
    "\n",
    "# Fit model for each target variable\n",
    "for target, y_train, y_test in [('Domain', y_train_domain, y_test_domain),\n",
    "                                 ('Software', y_train_software, y_test_software),\n",
    "                                 ('Platform', y_train_platform, y_test_platform)]:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(f\"Results for {target}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# To see the best parameters found by GridSearchCV for each target\n",
    "print(\"Best parameters found for Domain:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ce277997-27f8-4b43-bb01-a247b1c1cd37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [215, 200]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[190], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     28\u001b[0m X_train, y_train_domain \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train_domain)\n\u001b[0;32m---> 29\u001b[0m X_train, y_train_software \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train_software)\n\u001b[1;32m     30\u001b[0m X_train, y_train_platform \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train_platform)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Define the model with class weighting\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/base.py:106\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    104\u001b[0m check_classification_targets(y)\n\u001b[1;32m    105\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[0;32m--> 106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/base.py:161\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[0;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[1;32m    159\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 161\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:1281\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1264\u001b[0m     X,\n\u001b[1;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1277\u001b[0m )\n\u001b[1;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m-> 1281\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [215, 200]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE  # For handling imbalanced classes\n",
    "\n",
    "# Assuming `sampled_data` is your DataFrame containing the required columns.\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train_domain = smote.fit_resample(X_train, y_train_domain)\n",
    "X_train, y_train_software = smote.fit_resample(X_train, y_train_software)\n",
    "X_train, y_train_platform = smote.fit_resample(X_train, y_train_platform)\n",
    "\n",
    "# Define the model with class weighting\n",
    "model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='f1_macro')  # Reduce cv to avoid warnings\n",
    "\n",
    "# Fit model for each target variable\n",
    "for target, y_train, y_test in [('Domain', y_train_domain, y_test_domain),\n",
    "                                 ('Software', y_train_software, y_test_software),\n",
    "                                 ('Platform', y_train_platform, y_test_platform)]:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(f\"Results for {target}:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))  # Control zero division behavior\n",
    "\n",
    "# To see the best parameters found by GridSearchCV for each target\n",
    "print(\"Best parameters found for Domain:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5fd26c24-e11a-4caf-a125-970e5bbd8041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Domain:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.42      0.71      0.53         7\n",
      "    Education       1.00      1.00      1.00         9\n",
      "Entertainment       1.00      0.70      0.82        10\n",
      "      Finance       1.00      0.64      0.78        11\n",
      "        Media       0.67      0.77      0.71        13\n",
      "\n",
      "     accuracy                           0.76        50\n",
      "    macro avg       0.82      0.76      0.77        50\n",
      " weighted avg       0.83      0.76      0.78        50\n",
      "\n",
      "Results for Software:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.43      0.90      0.58        10\n",
      "       cloud       1.00      1.00      1.00        16\n",
      "    frontend       0.60      0.30      0.40        10\n",
      "      gaming       1.00      0.88      0.93         8\n",
      "      mobile       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.70        50\n",
      "   macro avg       0.61      0.61      0.58        50\n",
      "weighted avg       0.69      0.70      0.67        50\n",
      "\n",
      "Results for Platform:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.00      0.00      0.00         2\n",
      "           Android       0.36      0.40      0.38        10\n",
      "             Celia       0.00      0.00      0.00         3\n",
      "          Fanverse       0.00      0.00      0.00         2\n",
      "      Jio ChargeIT       0.10      0.50      0.17         2\n",
      "               Web       0.90      0.75      0.82        12\n",
      "               iOS       0.75      0.50      0.60        12\n",
      "streaming services       0.33      0.43      0.38         7\n",
      "\n",
      "          accuracy                           0.46        50\n",
      "         macro avg       0.31      0.32      0.29        50\n",
      "      weighted avg       0.52      0.46      0.48        50\n",
      "\n",
      "Best parameters found for Domain: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE  # For handling imbalanced classes\n",
    "\n",
    "# Assuming `sampled_data` is your DataFrame containing the required columns.\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define SMOTE with fewer neighbors\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)  # Reduce k_neighbors to 1 for safety\n",
    "\n",
    "# Function to apply SMOTE and check the number of samples\n",
    "def apply_smote(X_train, y_train):\n",
    "    if len(y_train.value_counts()) < 2:  # Check if there are fewer than 2 classes\n",
    "        print(\"Not enough classes for SMOTE.\")\n",
    "        return X_train, y_train  # Return original if not enough classes\n",
    "    return smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resample training set for each target variable\n",
    "X_train_domain, y_train_domain = smote.fit_resample(X_train, y_train_domain)\n",
    "X_train_software, y_train_software = smote.fit_resample(X_train, y_train_software)\n",
    "X_train_platform, y_train_platform = smote.fit_resample(X_train, y_train_platform)\n",
    "\n",
    "# Define the model with class weighting\n",
    "model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='f1_macro')  # Reduce cv to avoid warnings\n",
    "\n",
    "# Fit model for each target variable\n",
    "for target, X_train, y_train, y_test in [('Domain', X_train_domain, y_train_domain, y_test_domain),\n",
    "                                          ('Software', X_train_software, y_train_software, y_test_software),\n",
    "                                          ('Platform', X_train_platform, y_train_platform, y_test_platform)]:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(f\"Results for {target}:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))  # Control zero division behavior\n",
    "\n",
    "# To see the best parameters found by GridSearchCV for each target\n",
    "print(\"Best parameters found for Domain:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a189e084-f832-4274-839e-3a3f525fca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['User Stories', 'Description',\n",
      "       'Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]',\n",
      "       'Software Application [frontent, backend, mobile, desktop, cloud, gaming]',\n",
      "       'Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]',\n",
      "       'combined_text', 'predicted_Domain', 'predicted_Software',\n",
      "       'predicted_Platform'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(sampled_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9f49d27c-befb-475a-88b3-53cc859d72b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/kmq275hx6xvflqnbfp5t2vk80000gp/T/ipykernel_98352/3845267513.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Domain:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.73      0.89      0.80         9\n",
      "    Education       1.00      0.91      0.95        11\n",
      "Entertainment       1.00      1.00      1.00         7\n",
      "      Finance       0.90      0.82      0.86        11\n",
      "        Media       0.70      0.70      0.70        10\n",
      "\n",
      "     accuracy                           0.85        48\n",
      "    macro avg       0.87      0.86      0.86        48\n",
      " weighted avg       0.86      0.85      0.86        48\n",
      "\n",
      "Results for Software:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.60      0.75      0.67        12\n",
      "       cloud       0.94      1.00      0.97        17\n",
      "    frontend       1.00      0.29      0.44         7\n",
      "      gaming       1.00      0.86      0.92         7\n",
      "      mobile       0.43      0.60      0.50         5\n",
      "\n",
      "    accuracy                           0.77        48\n",
      "   macro avg       0.79      0.70      0.70        48\n",
      "weighted avg       0.82      0.77      0.76        48\n",
      "\n",
      "Results for Platform:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.50      1.00      0.67         3\n",
      "           Android       0.86      0.50      0.63        12\n",
      "      Jio ChargeIT       0.67      0.50      0.57         4\n",
      "         Playstore       0.00      0.00      0.00         1\n",
      "               Web       1.00      0.77      0.87        13\n",
      "               iOS       0.54      0.78      0.64         9\n",
      "streaming services       0.33      0.50      0.40         6\n",
      "\n",
      "          accuracy                           0.65        48\n",
      "         macro avg       0.56      0.58      0.54        48\n",
      "      weighted avg       0.71      0.65      0.65        48\n",
      "\n",
      "Best parameters found for Domain: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE  # For handling imbalanced classes\n",
    "\n",
    "# Define the categories to drop\n",
    "categories_to_drop = ['mobile', 'Celia', 'Fanverse']\n",
    "\n",
    "# Filter the dataset to drop the specified categories from 'Platform'\n",
    "sampled_data = sampled_data[~sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].isin(categories_to_drop)]\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define SMOTE with fewer neighbors\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)  # Reduce k_neighbors to 1 for safety\n",
    "\n",
    "# Function to apply SMOTE and check the number of samples\n",
    "def apply_smote(X_train, y_train):\n",
    "    if len(y_train.value_counts()) < 2:  # Check if there are fewer than 2 classes\n",
    "        print(\"Not enough classes for SMOTE.\")\n",
    "        return X_train, y_train  # Return original if not enough classes\n",
    "    return smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resample training set for each target variable\n",
    "X_train_domain, y_train_domain = apply_smote(X_train, y_train_domain)\n",
    "X_train_software, y_train_software = apply_smote(X_train, y_train_software)\n",
    "X_train_platform, y_train_platform = apply_smote(X_train, y_train_platform)\n",
    "\n",
    "# Define the model with class weighting\n",
    "model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='f1_macro')  # Reduce cv to avoid warnings\n",
    "\n",
    "# Fit model for each target variable\n",
    "for target, X_train, y_train, y_test in [('Domain', X_train_domain, y_train_domain, y_test_domain),\n",
    "                                          ('Software', X_train_software, y_train_software, y_test_software),\n",
    "                                          ('Platform', X_train_platform, y_train_platform, y_test_platform)]:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(f\"Results for {target}:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))  # Control zero division behavior\n",
    "\n",
    "# To see the best parameters found by GridSearchCV for each target\n",
    "print(\"Best parameters found for Domain:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a5c95f5d-0df7-4040-9f06-e3526009ebb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Domain:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.50      0.86      0.63         7\n",
      "    Education       1.00      0.90      0.95        10\n",
      "Entertainment       1.00      0.55      0.71        11\n",
      "      Finance       1.00      1.00      1.00        11\n",
      "        Media       0.56      0.62      0.59         8\n",
      "\n",
      "     accuracy                           0.79        47\n",
      "    macro avg       0.81      0.79      0.77        47\n",
      " weighted avg       0.85      0.79      0.80        47\n",
      "\n",
      "Results for Software:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.50      0.89      0.64         9\n",
      "       cloud       1.00      1.00      1.00        18\n",
      "    frontend       0.40      0.29      0.33         7\n",
      "      gaming       1.00      0.67      0.80         9\n",
      "      mobile       0.50      0.25      0.33         4\n",
      "\n",
      "    accuracy                           0.74        47\n",
      "   macro avg       0.68      0.62      0.62        47\n",
      "weighted avg       0.77      0.74      0.74        47\n",
      "\n",
      "Results for Platform:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.75      0.60      0.67         5\n",
      "           Android       0.89      0.57      0.70        14\n",
      "      Jio ChargeIT       0.25      1.00      0.40         1\n",
      "               Web       1.00      0.67      0.80        12\n",
      "               iOS       0.69      1.00      0.82         9\n",
      "streaming services       0.33      0.50      0.40         6\n",
      "\n",
      "          accuracy                           0.68        47\n",
      "         macro avg       0.65      0.72      0.63        47\n",
      "      weighted avg       0.78      0.68      0.70        47\n",
      "\n",
      "Best parameters found for Domain: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE  # For handling imbalanced classes\n",
    "\n",
    "# Define the categories to drop\n",
    "categories_to_drop = ['Playstore', '']\n",
    "\n",
    "# Filter the dataset to drop the specified categories from 'Platform'\n",
    "sampled_data = sampled_data[~sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].isin(categories_to_drop)]\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define SMOTE with fewer neighbors\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)  # Reduce k_neighbors to 1 for safety\n",
    "\n",
    "# Function to apply SMOTE and check the number of samples\n",
    "def apply_smote(X_train, y_train):\n",
    "    if len(y_train.value_counts()) < 2:  # Check if there are fewer than 2 classes\n",
    "        print(\"Not enough classes for SMOTE.\")\n",
    "        return X_train, y_train  # Return original if not enough classes\n",
    "    return smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resample training set for each target variable\n",
    "X_train_domain, y_train_domain = apply_smote(X_train, y_train_domain)\n",
    "X_train_software, y_train_software = apply_smote(X_train, y_train_software)\n",
    "X_train_platform, y_train_platform = apply_smote(X_train, y_train_platform)\n",
    "\n",
    "# Define the model with class weighting\n",
    "model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='f1_macro')  # Reduce cv to avoid warnings\n",
    "\n",
    "# Fit model for each target variable\n",
    "for target, X_train, y_train, y_test in [('Domain', X_train_domain, y_train_domain, y_test_domain),\n",
    "                                          ('Software', X_train_software, y_train_software, y_test_software),\n",
    "                                          ('Platform', X_train_platform, y_train_platform, y_test_platform)]:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(f\"Results for {target}:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))  # Control zero division behavior\n",
    "\n",
    "# To see the best parameters found by GridSearchCV for each target\n",
    "print(\"Best parameters found for Domain:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b193866a-a7e6-4376-a0fb-fa0fc4237455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231, 9)\n"
     ]
    }
   ],
   "source": [
    "print(sampled_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "cdd7f142-eaed-490a-a067-0623657e0a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (556, 384), y1_train shape: (556,)\n",
      "X_train_y2 shape: (556, 384), y2_train shape: (556,)\n",
      "X_train_y3 shape: (556, 384), y3_train shape: (556,)\n",
      "\n",
      "--- Evaluating models for y1 (Domain) ---\n",
      "Random Forest Accuracy: 0.9429\n",
      "Gradient Boosting Accuracy: 0.9214\n",
      "Support Vector Classifier Accuracy: 0.9429\n",
      "Logistic Regression Accuracy: 0.9429\n",
      "\n",
      "--- Evaluating models for y2 (Software) ---\n",
      "Random Forest Accuracy: 0.3643\n",
      "Gradient Boosting Accuracy: 0.3214\n",
      "Support Vector Classifier Accuracy: 0.3571\n",
      "Logistic Regression Accuracy: 0.3714\n",
      "\n",
      "--- Evaluating models for y3 (Platform) ---\n",
      "Random Forest Accuracy: 0.2643\n",
      "Gradient Boosting Accuracy: 0.2500\n",
      "Support Vector Classifier Accuracy: 0.2643\n",
      "Logistic Regression Accuracy: 0.2214\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "cleaned_data['combined_text'] = cleaned_data['combined_text'].fillna('')\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose another model if needed\n",
    "\n",
    "# Create the feature matrix X from combined text using sentence-transformer\n",
    "# The embeddings are stored in X\n",
    "X = model.encode(cleaned_data['combined_text'].tolist())\n",
    "\n",
    "# Create the target DataFrame\n",
    "target_data = pd.DataFrame({\n",
    "    'y1': y1,\n",
    "    'y2': y2,\n",
    "    'y3_cleaned': y3_cleaned\n",
    "})\n",
    "\n",
    "# Concatenate features and targets\n",
    "data_with_targets = pd.DataFrame(X)\n",
    "data_with_targets['y1'] = target_data['y1']\n",
    "data_with_targets['y2'] = target_data['y2']\n",
    "data_with_targets['y3_cleaned'] = target_data['y3_cleaned']\n",
    "\n",
    "# Drop rows with any missing values in the target columns\n",
    "data_with_targets = data_with_targets.dropna()\n",
    "\n",
    "# Recreate the feature matrix X and the target variables after alignment\n",
    "X_cleaned = data_with_targets.iloc[:, :-3].values  # All columns except last three (targets)\n",
    "y1_encoded = data_with_targets['y1'].values\n",
    "y2_encoded = data_with_targets['y2'].values\n",
    "y3_cleaned_encoded = data_with_targets['y3_cleaned'].values\n",
    "\n",
    "# Split for Domain (y1)\n",
    "X_train, X_test, y1_train, y1_test = train_test_split(X_cleaned, y1_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split for Software (y2)\n",
    "X_train_y2, X_test_y2, y2_train, y2_test = train_test_split(X_cleaned, y2_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split for Platform (y3_cleaned)\n",
    "X_train_y3, X_test_y3, y3_train, y3_test = train_test_split(X_cleaned, y3_cleaned_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display shapes to verify everything is correct\n",
    "print(f\"X_train shape: {X_train.shape}, y1_train shape: {y1_train.shape}\")\n",
    "print(f\"X_train_y2 shape: {X_train_y2.shape}, y2_train shape: {y2_train.shape}\")\n",
    "print(f\"X_train_y3 shape: {X_train_y3.shape}, y3_train shape: {y3_train.shape}\")\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Support Vector Classifier': SVC(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results[model_name] = accuracy\n",
    "        print(f'{model_name} Accuracy: {accuracy:.4f}')\n",
    "    return results\n",
    "\n",
    "# Train and evaluate for each target variable\n",
    "print(\"\\n--- Evaluating models for y1 (Domain) ---\")\n",
    "results_y1 = evaluate_models(X_train, X_test, y1_train, y1_test)\n",
    "\n",
    "print(\"\\n--- Evaluating models for y2 (Software) ---\")\n",
    "results_y2 = evaluate_models(X_train_y2, X_test_y2, y2_train, y2_test)\n",
    "\n",
    "print(\"\\n--- Evaluating models for y3 (Platform) ---\")\n",
    "results_y3 = evaluate_models(X_train_y3, X_test_y3, y3_train, y3_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "76e21420-65e3-4149-9c4f-03985723efae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Domain:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.50      0.86      0.63         7\n",
      "    Education       1.00      0.90      0.95        10\n",
      "Entertainment       1.00      0.55      0.71        11\n",
      "      Finance       1.00      1.00      1.00        11\n",
      "        Media       0.56      0.62      0.59         8\n",
      "\n",
      "     accuracy                           0.79        47\n",
      "    macro avg       0.81      0.79      0.77        47\n",
      " weighted avg       0.85      0.79      0.80        47\n",
      "\n",
      "Results for Software:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.50      0.89      0.64         9\n",
      "       cloud       1.00      1.00      1.00        18\n",
      "    frontend       0.40      0.29      0.33         7\n",
      "      gaming       1.00      0.67      0.80         9\n",
      "      mobile       0.50      0.25      0.33         4\n",
      "\n",
      "    accuracy                           0.74        47\n",
      "   macro avg       0.68      0.62      0.62        47\n",
      "weighted avg       0.77      0.74      0.74        47\n",
      "\n",
      "Results for Platform:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.75      0.60      0.67         5\n",
      "           Android       0.89      0.57      0.70        14\n",
      "      Jio ChargeIT       0.25      1.00      0.40         1\n",
      "               Web       1.00      0.67      0.80        12\n",
      "               iOS       0.69      1.00      0.82         9\n",
      "streaming services       0.33      0.50      0.40         6\n",
      "\n",
      "          accuracy                           0.68        47\n",
      "         macro avg       0.65      0.72      0.63        47\n",
      "      weighted avg       0.78      0.68      0.70        47\n",
      "\n",
      "Best parameters found for Domain: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "file_path = 'sampled_cleaned_data.csv'  # Replace with the actual path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming 'cleaned_data' is your DataFrame and 'Platform' is the column with platform values\n",
    "streaming_values = [\"Jio Cinema\", \"VOOT\", \"Playback\", \"ATV\", \"All Platforms\", \"AndroidTV\", \"Jio STB\", \n",
    "                    \"H5TV\", \"JioTV\", \"AppleTV\", \"CTV\", \"All platforms\"]\n",
    "\n",
    "# Replace the specified values with 'streaming services'\n",
    "sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'] = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].replace(streaming_values, 'streaming services')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE  # For handling imbalanced classes\n",
    "\n",
    "# Define the categories to drop\n",
    "categories_to_drop = ['mobile', 'Celia', 'Fanverse']\n",
    "\n",
    "# Filter the dataset to drop the specified categories from 'Platform'\n",
    "sampled_data = sampled_data[~sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].isin(categories_to_drop)]\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define SMOTE with fewer neighbors\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)  # Reduce k_neighbors to 1 for safety\n",
    "\n",
    "# Function to apply SMOTE and check the number of samples\n",
    "def apply_smote(X_train, y_train):\n",
    "    if len(y_train.value_counts()) < 2:  # Check if there are fewer than 2 classes\n",
    "        print(\"Not enough classes for SMOTE.\")\n",
    "        return X_train, y_train  # Return original if not enough classes\n",
    "    return smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resample training set for each target variable\n",
    "X_train_domain, y_train_domain = apply_smote(X_train, y_train_domain)\n",
    "X_train_software, y_train_software = apply_smote(X_train, y_train_software)\n",
    "X_train_platform, y_train_platform = apply_smote(X_train, y_train_platform)\n",
    "\n",
    "# Define the model with class weighting\n",
    "model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='f1_macro')  # Reduce cv to avoid warnings\n",
    "\n",
    "# Fit model for each target variable\n",
    "for target, X_train, y_train, y_test in [('Domain', X_train_domain, y_train_domain, y_test_domain),\n",
    "                                          ('Software', X_train_software, y_train_software, y_test_software),\n",
    "                                          ('Platform', X_train_platform, y_train_platform, y_test_platform)]:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(f\"Results for {target}:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))  # Control zero division behavior\n",
    "\n",
    "# To see the best parameters found by GridSearchCV for each target\n",
    "print(\"Best parameters found for Domain:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "81df0dea-23cf-4f7b-a955-09ecac11ec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Random Forest...\n",
      "\n",
      "Evaluating Domain with Random Forest\n",
      "Results for Domain with Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.50      0.86      0.63         7\n",
      "    Education       1.00      0.90      0.95        10\n",
      "Entertainment       1.00      0.55      0.71        11\n",
      "      Finance       1.00      1.00      1.00        11\n",
      "        Media       0.56      0.62      0.59         8\n",
      "\n",
      "     accuracy                           0.79        47\n",
      "    macro avg       0.81      0.79      0.77        47\n",
      " weighted avg       0.85      0.79      0.80        47\n",
      "\n",
      "Best parameters found for Domain with Random Forest: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "Evaluating Software with Random Forest\n",
      "Results for Software with Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.50      0.89      0.64         9\n",
      "       cloud       1.00      1.00      1.00        18\n",
      "    frontend       0.40      0.29      0.33         7\n",
      "      gaming       1.00      0.67      0.80         9\n",
      "      mobile       0.50      0.25      0.33         4\n",
      "\n",
      "    accuracy                           0.74        47\n",
      "   macro avg       0.68      0.62      0.62        47\n",
      "weighted avg       0.77      0.74      0.74        47\n",
      "\n",
      "Best parameters found for Software with Random Forest: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "Evaluating Platform with Random Forest\n",
      "Results for Platform with Random Forest:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.75      0.60      0.67         5\n",
      "           Android       0.89      0.57      0.70        14\n",
      "      Jio ChargeIT       0.25      1.00      0.40         1\n",
      "               Web       1.00      0.67      0.80        12\n",
      "               iOS       0.69      1.00      0.82         9\n",
      "streaming services       0.33      0.50      0.40         6\n",
      "\n",
      "          accuracy                           0.68        47\n",
      "         macro avg       0.65      0.72      0.63        47\n",
      "      weighted avg       0.78      0.68      0.70        47\n",
      "\n",
      "Best parameters found for Platform with Random Forest: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Training and evaluating Gradient Boosting...\n",
      "\n",
      "Evaluating Domain with Gradient Boosting\n",
      "Results for Domain with Gradient Boosting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.67      0.86      0.75         7\n",
      "    Education       0.77      1.00      0.87        10\n",
      "Entertainment       1.00      0.55      0.71        11\n",
      "      Finance       1.00      0.91      0.95        11\n",
      "        Media       0.67      0.75      0.71         8\n",
      "\n",
      "     accuracy                           0.81        47\n",
      "    macro avg       0.82      0.81      0.80        47\n",
      " weighted avg       0.84      0.81      0.80        47\n",
      "\n",
      "Best parameters found for Domain with Gradient Boosting: {'max_depth': 3, 'n_estimators': 100}\n",
      "\n",
      "Evaluating Software with Gradient Boosting\n",
      "Results for Software with Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.55      0.67      0.60         9\n",
      "       cloud       1.00      0.94      0.97        18\n",
      "    frontend       0.43      0.43      0.43         7\n",
      "      gaming       0.86      0.67      0.75         9\n",
      "      mobile       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.77        47\n",
      "   macro avg       0.73      0.74      0.73        47\n",
      "weighted avg       0.78      0.77      0.77        47\n",
      "\n",
      "Best parameters found for Software with Gradient Boosting: {'max_depth': 5, 'n_estimators': 100}\n",
      "\n",
      "Evaluating Platform with Gradient Boosting\n",
      "Results for Platform with Gradient Boosting:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.75      0.60      0.67         5\n",
      "           Android       1.00      0.57      0.73        14\n",
      "      Jio ChargeIT       0.33      1.00      0.50         1\n",
      "               Web       1.00      0.75      0.86        12\n",
      "               iOS       0.64      1.00      0.78         9\n",
      "streaming services       0.44      0.67      0.53         6\n",
      "\n",
      "          accuracy                           0.72        47\n",
      "         macro avg       0.70      0.76      0.68        47\n",
      "      weighted avg       0.82      0.72      0.73        47\n",
      "\n",
      "Best parameters found for Platform with Gradient Boosting: {'max_depth': 3, 'n_estimators': 100}\n",
      "Training and evaluating Support Vector Classifier...\n",
      "\n",
      "Evaluating Domain with Support Vector Classifier\n",
      "Results for Domain with Support Vector Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.67      0.57      0.62         7\n",
      "    Education       1.00      0.80      0.89        10\n",
      "Entertainment       1.00      0.55      0.71        11\n",
      "      Finance       0.92      1.00      0.96        11\n",
      "        Media       0.47      0.88      0.61         8\n",
      "\n",
      "     accuracy                           0.77        47\n",
      "    macro avg       0.81      0.76      0.76        47\n",
      " weighted avg       0.84      0.77      0.77        47\n",
      "\n",
      "Best parameters found for Domain with Support Vector Classifier: {'C': 10, 'kernel': 'rbf'}\n",
      "\n",
      "Evaluating Software with Support Vector Classifier\n",
      "Results for Software with Support Vector Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.42      0.89      0.57         9\n",
      "       cloud       1.00      1.00      1.00        18\n",
      "    frontend       0.50      0.14      0.22         7\n",
      "      gaming       1.00      0.67      0.80         9\n",
      "      mobile       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.70        47\n",
      "   macro avg       0.58      0.54      0.52        47\n",
      "weighted avg       0.73      0.70      0.68        47\n",
      "\n",
      "Best parameters found for Software with Support Vector Classifier: {'C': 1, 'kernel': 'rbf'}\n",
      "\n",
      "Evaluating Platform with Support Vector Classifier\n",
      "Results for Platform with Support Vector Classifier:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.00      0.00      0.00         5\n",
      "           Android       0.42      0.79      0.55        14\n",
      "      Jio ChargeIT       0.00      0.00      0.00         1\n",
      "               Web       0.89      0.67      0.76        12\n",
      "               iOS       0.80      0.44      0.57         9\n",
      "streaming services       0.00      0.00      0.00         6\n",
      "\n",
      "          accuracy                           0.49        47\n",
      "         macro avg       0.35      0.32      0.31        47\n",
      "      weighted avg       0.51      0.49      0.47        47\n",
      "\n",
      "Best parameters found for Platform with Support Vector Classifier: {'C': 10, 'kernel': 'rbf'}\n",
      "Training and evaluating Logistic Regression...\n",
      "\n",
      "Evaluating Domain with Logistic Regression\n",
      "Results for Domain with Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.67      0.57      0.62         7\n",
      "    Education       1.00      0.80      0.89        10\n",
      "Entertainment       1.00      0.55      0.71        11\n",
      "      Finance       1.00      1.00      1.00        11\n",
      "        Media       0.44      0.88      0.58         8\n",
      "\n",
      "     accuracy                           0.77        47\n",
      "    macro avg       0.82      0.76      0.76        47\n",
      " weighted avg       0.85      0.77      0.78        47\n",
      "\n",
      "Best parameters found for Domain with Logistic Regression: {'C': 0.1}\n",
      "\n",
      "Evaluating Software with Logistic Regression\n",
      "Results for Software with Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.47      0.78      0.58         9\n",
      "       cloud       1.00      1.00      1.00        18\n",
      "    frontend       0.25      0.14      0.18         7\n",
      "      gaming       1.00      0.67      0.80         9\n",
      "      mobile       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.54      0.52      0.51        47\n",
      "weighted avg       0.70      0.68      0.67        47\n",
      "\n",
      "Best parameters found for Software with Logistic Regression: {'C': 10}\n",
      "\n",
      "Evaluating Platform with Logistic Regression\n",
      "Results for Platform with Logistic Regression:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.80      0.80      0.80         5\n",
      "           Android       0.64      0.50      0.56        14\n",
      "      Jio ChargeIT       0.00      0.00      0.00         1\n",
      "               Web       0.89      0.67      0.76        12\n",
      "               iOS       0.58      0.78      0.67         9\n",
      "streaming services       0.40      0.33      0.36         6\n",
      "\n",
      "          accuracy                           0.60        47\n",
      "         macro avg       0.55      0.51      0.53        47\n",
      "      weighted avg       0.66      0.60      0.62        47\n",
      "\n",
      "Best parameters found for Platform with Logistic Regression: {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Define the categories to drop\n",
    "categories_to_drop = ['mobile', 'Celia', 'Fanverse']\n",
    "\n",
    "# Filter the dataset to drop the specified categories from 'Platform'\n",
    "sampled_data = sampled_data[~sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].isin(categories_to_drop)]\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define SMOTE with fewer neighbors\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "\n",
    "# Function to apply SMOTE and check the number of samples\n",
    "def apply_smote(X_train, y_train):\n",
    "    if len(y_train.value_counts()) < 2:  # Check if there are fewer than 2 classes\n",
    "        print(\"Not enough classes for SMOTE.\")\n",
    "        return X_train, y_train  # Return original if not enough classes\n",
    "    return smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resample training set for each target variable\n",
    "X_train_domain, y_train_domain = apply_smote(X_train, y_train_domain)\n",
    "X_train_software, y_train_software = apply_smote(X_train, y_train_software)\n",
    "X_train_platform, y_train_platform = apply_smote(X_train, y_train_platform)\n",
    "\n",
    "# Define the model dictionary\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Support Vector Classifier': SVC(random_state=42, class_weight='balanced'),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "}\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning for each model\n",
    "param_grids = {\n",
    "    'Random Forest': {'n_estimators': [50, 100], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5]},\n",
    "    'Gradient Boosting': {'n_estimators': [50, 100], 'max_depth': [3, 5, 10]},\n",
    "    'Support Vector Classifier': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]}\n",
    "}\n",
    "\n",
    "# Loop through the models and perform training, tuning, and evaluation\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "\n",
    "    for target, X_train, y_train, y_test in [\n",
    "        ('Domain', X_train_domain, y_train_domain, y_test_domain),\n",
    "        ('Software', X_train_software, y_train_software, y_test_software),\n",
    "        ('Platform', X_train_platform, y_train_platform, y_test_platform)\n",
    "    ]:\n",
    "        print(f\"\\nEvaluating {target} with {model_name}\")\n",
    "\n",
    "        # Define GridSearchCV for each model\n",
    "        grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='f1_macro')\n",
    "\n",
    "        # Fit the model\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        print(f\"Results for {target} with {model_name}:\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "        # Print best parameters for each model\n",
    "        print(f\"Best parameters found for {target} with {model_name}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "49acaff5-77a6-40f4-a8fa-129787456089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0e5b0987c7497fa1ed20cfc276db6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Random Forest...\n",
      "\n",
      "Evaluating Domain with Random Forest\n",
      "Results for Domain with Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.83      0.71      0.77         7\n",
      "    Education       0.90      0.90      0.90        10\n",
      "Entertainment       1.00      0.64      0.78        11\n",
      "      Finance       1.00      1.00      1.00        11\n",
      "        Media       0.46      0.75      0.57         8\n",
      "\n",
      "     accuracy                           0.81        47\n",
      "    macro avg       0.84      0.80      0.80        47\n",
      " weighted avg       0.86      0.81      0.82        47\n",
      "\n",
      "Best parameters found for Domain with Random Forest: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "Evaluating Software with Random Forest\n",
      "Results for Software with Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.58      0.78      0.67         9\n",
      "       cloud       0.86      1.00      0.92        18\n",
      "    frontend       0.67      0.29      0.40         7\n",
      "      gaming       1.00      0.78      0.88         9\n",
      "      mobile       0.25      0.25      0.25         4\n",
      "\n",
      "    accuracy                           0.74        47\n",
      "   macro avg       0.67      0.62      0.62        47\n",
      "weighted avg       0.75      0.74      0.73        47\n",
      "\n",
      "Best parameters found for Software with Random Forest: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "\n",
      "Evaluating Platform with Random Forest\n",
      "Results for Platform with Random Forest:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.50      0.20      0.29         5\n",
      "           Android       0.62      0.71      0.67        14\n",
      "      Jio ChargeIT       0.00      0.00      0.00         1\n",
      "               Web       0.82      0.75      0.78        12\n",
      "               iOS       0.64      0.78      0.70         9\n",
      "streaming services       0.33      0.33      0.33         6\n",
      "\n",
      "          accuracy                           0.62        47\n",
      "         macro avg       0.49      0.46      0.46        47\n",
      "      weighted avg       0.61      0.62      0.61        47\n",
      "\n",
      "Best parameters found for Platform with Random Forest: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Training and evaluating Gradient Boosting...\n",
      "\n",
      "Evaluating Domain with Gradient Boosting\n",
      "Results for Domain with Gradient Boosting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.86      0.86      0.86         7\n",
      "    Education       0.82      0.90      0.86        10\n",
      "Entertainment       0.88      0.64      0.74        11\n",
      "      Finance       1.00      0.91      0.95        11\n",
      "        Media       0.55      0.75      0.63         8\n",
      "\n",
      "     accuracy                           0.81        47\n",
      "    macro avg       0.82      0.81      0.81        47\n",
      " weighted avg       0.83      0.81      0.81        47\n",
      "\n",
      "Best parameters found for Domain with Gradient Boosting: {'max_depth': 3, 'n_estimators': 100}\n",
      "\n",
      "Evaluating Software with Gradient Boosting\n",
      "Results for Software with Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.00      0.00      0.00         0\n",
      "     backend       0.33      0.44      0.38         9\n",
      "       cloud       0.94      0.89      0.91        18\n",
      "    frontend       1.00      0.14      0.25         7\n",
      "      gaming       0.83      0.56      0.67         9\n",
      "      mobile       0.25      0.50      0.33         4\n",
      "\n",
      "    accuracy                           0.60        47\n",
      "   macro avg       0.56      0.42      0.42        47\n",
      "weighted avg       0.75      0.60      0.62        47\n",
      "\n",
      "Best parameters found for Software with Gradient Boosting: {'max_depth': 3, 'n_estimators': 100}\n",
      "\n",
      "Evaluating Platform with Gradient Boosting\n",
      "Results for Platform with Gradient Boosting:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.33      0.20      0.25         5\n",
      "           Android       0.67      0.71      0.69        14\n",
      "      Jio ChargeIT       0.00      0.00      0.00         1\n",
      "               Web       0.90      0.75      0.82        12\n",
      "               iOS       0.64      0.78      0.70         9\n",
      "streaming services       0.29      0.33      0.31         6\n",
      "\n",
      "          accuracy                           0.62        47\n",
      "         macro avg       0.47      0.46      0.46        47\n",
      "      weighted avg       0.62      0.62      0.61        47\n",
      "\n",
      "Best parameters found for Platform with Gradient Boosting: {'max_depth': 3, 'n_estimators': 100}\n",
      "Training and evaluating Support Vector Classifier...\n",
      "\n",
      "Evaluating Domain with Support Vector Classifier\n",
      "Results for Domain with Support Vector Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.67      0.86      0.75         7\n",
      "    Education       1.00      0.90      0.95        10\n",
      "Entertainment       1.00      0.64      0.78        11\n",
      "      Finance       1.00      1.00      1.00        11\n",
      "        Media       0.55      0.75      0.63         8\n",
      "\n",
      "     accuracy                           0.83        47\n",
      "    macro avg       0.84      0.83      0.82        47\n",
      " weighted avg       0.87      0.83      0.84        47\n",
      "\n",
      "Best parameters found for Domain with Support Vector Classifier: {'C': 10, 'kernel': 'rbf'}\n",
      "\n",
      "Evaluating Software with Support Vector Classifier\n",
      "Results for Software with Support Vector Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.50      0.78      0.61         9\n",
      "       cloud       1.00      1.00      1.00        18\n",
      "    frontend       0.50      0.29      0.36         7\n",
      "      gaming       1.00      0.78      0.88         9\n",
      "      mobile       0.25      0.25      0.25         4\n",
      "\n",
      "    accuracy                           0.74        47\n",
      "   macro avg       0.65      0.62      0.62        47\n",
      "weighted avg       0.77      0.74      0.74        47\n",
      "\n",
      "Best parameters found for Software with Support Vector Classifier: {'C': 1, 'kernel': 'rbf'}\n",
      "\n",
      "Evaluating Platform with Support Vector Classifier\n",
      "Results for Platform with Support Vector Classifier:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         1.00      0.40      0.57         5\n",
      "           Android       0.62      0.57      0.59        14\n",
      "      Jio ChargeIT       0.00      0.00      0.00         1\n",
      "               Web       1.00      0.75      0.86        12\n",
      "               iOS       0.58      0.78      0.67         9\n",
      "streaming services       0.22      0.33      0.27         6\n",
      "\n",
      "          accuracy                           0.60        47\n",
      "         macro avg       0.57      0.47      0.49        47\n",
      "      weighted avg       0.69      0.60      0.62        47\n",
      "\n",
      "Best parameters found for Platform with Support Vector Classifier: {'C': 10, 'kernel': 'rbf'}\n",
      "Training and evaluating Logistic Regression...\n",
      "\n",
      "Evaluating Domain with Logistic Regression\n",
      "Results for Domain with Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.67      0.86      0.75         7\n",
      "    Education       1.00      0.90      0.95        10\n",
      "Entertainment       1.00      0.64      0.78        11\n",
      "      Finance       1.00      1.00      1.00        11\n",
      "        Media       0.55      0.75      0.63         8\n",
      "\n",
      "     accuracy                           0.83        47\n",
      "    macro avg       0.84      0.83      0.82        47\n",
      " weighted avg       0.87      0.83      0.84        47\n",
      "\n",
      "Best parameters found for Domain with Logistic Regression: {'C': 10}\n",
      "\n",
      "Evaluating Software with Logistic Regression\n",
      "Results for Software with Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.56      0.56      0.56         9\n",
      "       cloud       1.00      1.00      1.00        18\n",
      "    frontend       0.20      0.14      0.17         7\n",
      "      gaming       1.00      0.78      0.88         9\n",
      "      mobile       0.38      0.75      0.50         4\n",
      "\n",
      "    accuracy                           0.72        47\n",
      "   macro avg       0.63      0.65      0.62        47\n",
      "weighted avg       0.74      0.72      0.72        47\n",
      "\n",
      "Best parameters found for Software with Logistic Regression: {'C': 10}\n",
      "\n",
      "Evaluating Platform with Logistic Regression\n",
      "Results for Platform with Logistic Regression:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.67      0.40      0.50         5\n",
      "           Android       0.67      0.71      0.69        14\n",
      "      Jio ChargeIT       0.33      1.00      0.50         1\n",
      "               Web       1.00      0.75      0.86        12\n",
      "               iOS       0.57      0.89      0.70         9\n",
      "streaming services       0.33      0.17      0.22         6\n",
      "\n",
      "          accuracy                           0.66        47\n",
      "         macro avg       0.60      0.65      0.58        47\n",
      "      weighted avg       0.68      0.66      0.65        47\n",
      "\n",
      "Best parameters found for Platform with Logistic Regression: {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Define the categories to drop\n",
    "categories_to_drop = ['mobile', 'Celia', 'Fanverse']\n",
    "\n",
    "# Filter the dataset to drop the specified categories from 'Platform'\n",
    "sampled_data = sampled_data[~sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].isin(categories_to_drop)]\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'  # You can change this to another model if needed\n",
    "sentence_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Use the sentence transformer to encode the 'combined_text' column\n",
    "X = sentence_model.encode(sampled_data['combined_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define SMOTE with fewer neighbors\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "\n",
    "# Function to apply SMOTE and check the number of samples\n",
    "def apply_smote(X_train, y_train):\n",
    "    if len(y_train.value_counts()) < 2:  # Check if there are fewer than 2 classes\n",
    "        print(\"Not enough classes for SMOTE.\")\n",
    "        return X_train, y_train  # Return original if not enough classes\n",
    "    return smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resample training set for each target variable\n",
    "X_train_domain, y_train_domain = apply_smote(X_train, y_train_domain)\n",
    "X_train_software, y_train_software = apply_smote(X_train, y_train_software)\n",
    "X_train_platform, y_train_platform = apply_smote(X_train, y_train_platform)\n",
    "\n",
    "# Define the model dictionary\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Support Vector Classifier': SVC(random_state=42, class_weight='balanced'),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "}\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning for each model\n",
    "param_grids = {\n",
    "    'Random Forest': {'n_estimators': [50, 100], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5]},\n",
    "    'Gradient Boosting': {'n_estimators': [50, 100], 'max_depth': [3, 5, 10]},\n",
    "    'Support Vector Classifier': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]}\n",
    "}\n",
    "\n",
    "# Loop through the models and perform training, tuning, and evaluation\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "\n",
    "    for target, X_train, y_train, y_test in [\n",
    "        ('Domain', X_train_domain, y_train_domain, y_test_domain),\n",
    "        ('Software', X_train_software, y_train_software, y_test_software),\n",
    "        ('Platform', X_train_platform, y_train_platform, y_test_platform)\n",
    "    ]:\n",
    "        print(f\"\\nEvaluating {target} with {model_name}\")\n",
    "\n",
    "        # Define GridSearchCV for each model\n",
    "        grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='f1_macro')\n",
    "\n",
    "        # Fit the model\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        print(f\"Results for {target} with {model_name}:\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "        # Print best parameters for each model\n",
    "        print(f\"Best parameters found for {target} with {model_name}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "30e0c220-0029-4d00-9882-37c9ef692193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]\n",
       "Media            50\n",
       "Education        50\n",
       "E-commerce       47\n",
       "Finance          43\n",
       "Entertainment    41\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data[domain_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "3a4de5b7-4f28-4905-9843-f40460ce6114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Software Application [frontent, backend, mobile, desktop, cloud, gaming]\n",
       "cloud       82\n",
       "backend     60\n",
       "gaming      38\n",
       "mobile      26\n",
       "frontend    23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data[software_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "2545dc62-e8b6-4d19-8193-d3b2860c95de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]\n",
       "Android               60\n",
       "Web                   50\n",
       "iOS                   48\n",
       "streaming services    39\n",
       "Jio ChargeIT          12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "db606627-a053-49ec-9e05-7c0353be4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sampled_data as a CSV file\n",
    "sampled_data.to_csv('final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "54a962db-b6d1-4ce7-99dc-0589510ac9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Domain with Logistic Regression\n",
      "Results for Domain:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.67      0.57      0.62         7\n",
      "    Education       1.00      0.80      0.89        10\n",
      "Entertainment       1.00      0.55      0.71        11\n",
      "      Finance       1.00      1.00      1.00        11\n",
      "        Media       0.44      0.88      0.58         8\n",
      "\n",
      "     accuracy                           0.77        47\n",
      "    macro avg       0.82      0.76      0.76        47\n",
      " weighted avg       0.85      0.77      0.78        47\n",
      "\n",
      "Best parameters: {'C': 0.1}\n",
      "\n",
      "Evaluating Software with Gradient Boosting\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 384 features, but GradientBoostingClassifier is expecting 3156 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[258], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grids[model_name], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     74\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 75\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:546\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03m    the best found parameters.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    545\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:1611\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict class for X.\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m \n\u001b[1;32m   1599\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1611\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# decision_function already squeezed it\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m         encoded_classes \u001b[38;5;241m=\u001b[39m (raw_predictions \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:1564\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m \n\u001b[1;32m   1548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;124;03m        array of shape (n_samples,).\u001b[39;00m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1564\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1565\u001b[0m         X, dtype\u001b[38;5;241m=\u001b[39mDTYPE, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m     )\n\u001b[1;32m   1567\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_predict(X)\n\u001b[1;32m   1568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 384 features, but GradientBoostingClassifier is expecting 3156 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the final_data.csv\n",
    "sampled_data = pd.read_csv('final_data.csv')\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets for each target\n",
    "X_train_domain, X_test_domain, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train_software, X_test_software, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train_platform, X_test_platform, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define SMOTE with fewer neighbors\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "\n",
    "# Function to apply SMOTE and check the number of samples\n",
    "def apply_smote(X_train, y_train):\n",
    "    if len(y_train.value_counts()) < 2:  # Check if there are fewer than 2 classes\n",
    "        print(\"Not enough classes for SMOTE.\")\n",
    "        return X_train, y_train  # Return original if not enough classes\n",
    "    return smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resample training set for each target variable\n",
    "X_train_domain, y_train_domain = apply_smote(X_train_domain, y_train_domain)\n",
    "X_train_software, y_train_software = apply_smote(X_train_software, y_train_software)\n",
    "X_train_platform, y_train_platform = apply_smote(X_train_platform, y_train_platform)\n",
    "\n",
    "# Define the model dictionary with preferred models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "    'Gradient Boosting': {'n_estimators': [50, 100], 'max_depth': [3, 5, 10]}\n",
    "}\n",
    "\n",
    "# Train Logistic Regression for Domain, Gradient Boosting for Software and Platform\n",
    "for model_name, model in models.items():\n",
    "    if model_name == 'Logistic Regression':\n",
    "        print(f\"\\nEvaluating Domain with {model_name}\")\n",
    "        grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='f1_macro')\n",
    "        grid_search.fit(X_train_domain, y_train_domain)\n",
    "        y_pred = grid_search.predict(X_test_domain)\n",
    "        print(\"Results for Domain:\")\n",
    "        print(classification_report(y_test_domain, y_pred, zero_division=0))\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        for target, X_train, y_train, y_test in [\n",
    "            ('Software', X_train_software, y_train_software, y_test_software),\n",
    "            ('Platform', X_train_platform, y_train_platform, y_test_platform)\n",
    "        ]:\n",
    "            print(f\"\\nEvaluating {target} with {model_name}\")\n",
    "            grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='f1_macro')\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            y_pred = grid_search.predict(X_test)\n",
    "            print(f\"Results for {target}:\")\n",
    "            print(classification_report(y_test, y_pred, zero_division=0))\n",
    "            print(f\"Best parameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "faebfefc-f276-4598-aff5-1d2bf7dc0a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Gradient Boosting...\n",
      "\n",
      "Evaluating Domain with Gradient Boosting\n",
      "Results for Domain with Gradient Boosting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.67      0.86      0.75         7\n",
      "    Education       0.77      1.00      0.87        10\n",
      "Entertainment       1.00      0.55      0.71        11\n",
      "      Finance       1.00      0.91      0.95        11\n",
      "        Media       0.67      0.75      0.71         8\n",
      "\n",
      "     accuracy                           0.81        47\n",
      "    macro avg       0.82      0.81      0.80        47\n",
      " weighted avg       0.84      0.81      0.80        47\n",
      "\n",
      "Best parameters found for Domain with Gradient Boosting: {'max_depth': 3, 'n_estimators': 100}\n",
      "\n",
      "Evaluating Software with Gradient Boosting\n",
      "Results for Software with Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.55      0.67      0.60         9\n",
      "       cloud       1.00      0.94      0.97        18\n",
      "    frontend       0.43      0.43      0.43         7\n",
      "      gaming       0.86      0.67      0.75         9\n",
      "      mobile       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.77        47\n",
      "   macro avg       0.73      0.74      0.73        47\n",
      "weighted avg       0.78      0.77      0.77        47\n",
      "\n",
      "Best parameters found for Software with Gradient Boosting: {'max_depth': 5, 'n_estimators': 100}\n",
      "\n",
      "Evaluating Platform with Gradient Boosting\n",
      "Results for Platform with Gradient Boosting:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.75      0.60      0.67         5\n",
      "           Android       1.00      0.57      0.73        14\n",
      "      Jio ChargeIT       0.33      1.00      0.50         1\n",
      "               Web       1.00      0.75      0.86        12\n",
      "               iOS       0.64      1.00      0.78         9\n",
      "streaming services       0.44      0.67      0.53         6\n",
      "\n",
      "          accuracy                           0.72        47\n",
      "         macro avg       0.70      0.76      0.68        47\n",
      "      weighted avg       0.82      0.72      0.73        47\n",
      "\n",
      "Best parameters found for Platform with Gradient Boosting: {'max_depth': 3, 'n_estimators': 100}\n",
      "Training and evaluating Logistic Regression...\n",
      "\n",
      "Evaluating Domain with Logistic Regression\n",
      "Results for Domain with Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.67      0.57      0.62         7\n",
      "    Education       1.00      0.80      0.89        10\n",
      "Entertainment       1.00      0.55      0.71        11\n",
      "      Finance       1.00      1.00      1.00        11\n",
      "        Media       0.44      0.88      0.58         8\n",
      "\n",
      "     accuracy                           0.77        47\n",
      "    macro avg       0.82      0.76      0.76        47\n",
      " weighted avg       0.85      0.77      0.78        47\n",
      "\n",
      "Best parameters found for Domain with Logistic Regression: {'C': 0.1}\n",
      "\n",
      "Evaluating Software with Logistic Regression\n",
      "Results for Software with Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.47      0.78      0.58         9\n",
      "       cloud       1.00      1.00      1.00        18\n",
      "    frontend       0.25      0.14      0.18         7\n",
      "      gaming       1.00      0.67      0.80         9\n",
      "      mobile       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.68        47\n",
      "   macro avg       0.54      0.52      0.51        47\n",
      "weighted avg       0.70      0.68      0.67        47\n",
      "\n",
      "Best parameters found for Software with Logistic Regression: {'C': 10}\n",
      "\n",
      "Evaluating Platform with Logistic Regression\n",
      "Results for Platform with Logistic Regression:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.80      0.80      0.80         5\n",
      "           Android       0.64      0.50      0.56        14\n",
      "      Jio ChargeIT       0.00      0.00      0.00         1\n",
      "               Web       0.89      0.67      0.76        12\n",
      "               iOS       0.58      0.78      0.67         9\n",
      "streaming services       0.40      0.33      0.36         6\n",
      "\n",
      "          accuracy                           0.60        47\n",
      "         macro avg       0.55      0.51      0.53        47\n",
      "      weighted avg       0.66      0.60      0.62        47\n",
      "\n",
      "Best parameters found for Platform with Logistic Regression: {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load final_data from CSV\n",
    "sampled_data = pd.read_csv('final_data.csv')\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets once for features and domain\n",
    "X_train, X_test, y_domain_train, y_domain_test = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets for software and platform\n",
    "_, _, y_software_train, y_software_test = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "_, _, y_platform_train, y_platform_test = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "\n",
    "# Function to apply SMOTE\n",
    "def apply_smote(X_train, y_train):\n",
    "    if len(y_train.value_counts()) < 2:  # Check if there are fewer than 2 classes\n",
    "        print(\"Not enough classes for SMOTE.\")\n",
    "        return X_train, y_train  # Return original if not enough classes\n",
    "    return smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resample training set for each target variable\n",
    "X_train_domain, y_train_domain = apply_smote(X_train, y_domain_train)\n",
    "X_train_software, y_train_software = apply_smote(X_train, y_software_train)\n",
    "X_train_platform, y_train_platform = apply_smote(X_train, y_platform_train)\n",
    "\n",
    "# Define the model dictionary\n",
    "models = {\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "}\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning for each model\n",
    "param_grids = {\n",
    "    'Gradient Boosting': {'n_estimators': [50, 100], 'max_depth': [3, 5, 10]},\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]}\n",
    "}\n",
    "\n",
    "# Loop through the models and perform training, tuning, and evaluation\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "\n",
    "    for target, X_train, y_train, y_test in [\n",
    "        ('Domain', X_train_domain, y_train_domain, y_domain_test),\n",
    "        ('Software', X_train_software, y_train_software, y_software_test),\n",
    "        ('Platform', X_train_platform, y_train_platform, y_platform_test)\n",
    "    ]:\n",
    "        print(f\"\\nEvaluating {target} with {model_name}\")\n",
    "\n",
    "        # Define GridSearchCV for each model\n",
    "        grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='f1_macro')\n",
    "\n",
    "        # Fit the model\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        print(f\"Results for {target} with {model_name}:\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "        # Print best parameters for each model\n",
    "        print(f\"Best parameters found for {target} with {model_name}: {grid_search.best_params_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0ba661a1-d068-46c8-861d-1a9bf4cd4e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Logistic Regression...\n",
      "\n",
      "Evaluating Platform with Logistic Regression\n",
      "Results for Platform with Logistic Regression:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.80      0.80      0.80         5\n",
      "           Android       0.64      0.50      0.56        14\n",
      "      Jio ChargeIT       0.00      0.00      0.00         1\n",
      "               Web       0.89      0.67      0.76        12\n",
      "               iOS       0.58      0.78      0.67         9\n",
      "streaming services       0.40      0.33      0.36         6\n",
      "\n",
      "          accuracy                           0.60        47\n",
      "         macro avg       0.55      0.51      0.53        47\n",
      "      weighted avg       0.66      0.60      0.62        47\n",
      "\n",
      "Best parameters found for Platform with Logistic Regression: {'C': 10}\n",
      "Training and evaluating Gradient Boosting Software...\n",
      "\n",
      "Evaluating Software with Gradient Boosting Software\n",
      "Results for Software with Gradient Boosting Software:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.55      0.67      0.60         9\n",
      "       cloud       1.00      0.94      0.97        18\n",
      "    frontend       0.43      0.43      0.43         7\n",
      "      gaming       0.86      0.67      0.75         9\n",
      "      mobile       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.77        47\n",
      "   macro avg       0.73      0.74      0.73        47\n",
      "weighted avg       0.78      0.77      0.77        47\n",
      "\n",
      "Best parameters found for Software with Gradient Boosting Software: {'max_depth': 5, 'n_estimators': 100}\n",
      "Training and evaluating Gradient Boosting Platform...\n",
      "\n",
      "Evaluating Platform with Gradient Boosting Platform\n",
      "Results for Platform with Gradient Boosting Platform:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.75      0.60      0.67         5\n",
      "           Android       1.00      0.57      0.73        14\n",
      "      Jio ChargeIT       0.33      1.00      0.50         1\n",
      "               Web       1.00      0.75      0.86        12\n",
      "               iOS       0.64      1.00      0.78         9\n",
      "streaming services       0.44      0.67      0.53         6\n",
      "\n",
      "          accuracy                           0.72        47\n",
      "         macro avg       0.70      0.76      0.68        47\n",
      "      weighted avg       0.82      0.72      0.73        47\n",
      "\n",
      "Best parameters found for Platform with Gradient Boosting Platform: {'max_depth': 3, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the final dataset\n",
    "sampled_data = pd.read_csv('final_data.csv')\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define SMOTE with fewer neighbors\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "\n",
    "# Function to apply SMOTE and check the number of samples\n",
    "def apply_smote(X_train, y_train):\n",
    "    if len(y_train.value_counts()) < 2:  # Check if there are fewer than 2 classes\n",
    "        print(\"Not enough classes for SMOTE.\")\n",
    "        return X_train, y_train  # Return original if not enough classes\n",
    "    return smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resample training set for each target variable\n",
    "X_train_domain, y_train_domain = apply_smote(X_train, y_train_domain)\n",
    "X_train_software, y_train_software = apply_smote(X_train, y_train_software)\n",
    "X_train_platform, y_train_platform = apply_smote(X_train, y_train_platform)\n",
    "\n",
    "# Define the model dictionary\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),  # For Domain\n",
    "    'Gradient Boosting Software': GradientBoostingClassifier(random_state=42),  # For Software\n",
    "    'Gradient Boosting Platform': GradientBoostingClassifier(random_state=42)  # For Platform\n",
    "}\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "    'Gradient Boosting Software': {'n_estimators': [50, 100], 'max_depth': [3, 5, 10]},\n",
    "    'Gradient Boosting Platform': {'n_estimators': [50, 100], 'max_depth': [3, 5, 10]}\n",
    "}\n",
    "\n",
    "# Loop through the models and perform training, tuning, and evaluation\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "\n",
    "    # Determine the target variable\n",
    "    if \"Domain\" in model_name:\n",
    "        target, X_train, y_train, y_test = 'Domain', X_train_domain, y_train_domain, y_test_domain\n",
    "    else:\n",
    "        # For Gradient Boosting, determine the target based on model name\n",
    "        if \"Software\" in model_name:\n",
    "            target, X_train, y_train, y_test = 'Software', X_train_software, y_train_software, y_test_software\n",
    "        else:\n",
    "            target, X_train, y_train, y_test = 'Platform', X_train_platform, y_train_platform, y_test_platform\n",
    "\n",
    "    print(f\"\\nEvaluating {target} with {model_name}\")\n",
    "\n",
    "    # Define GridSearchCV for each model\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='f1_macro')\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    print(f\"Results for {target} with {model_name}:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # Print best parameters for each model\n",
    "    print(f\"Best parameters found for {target} with {model_name}: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2b21ccfb-d630-4371-a168-6b91f5f8c22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Logistic Regression...\n",
      "\n",
      "Evaluating Domain with Logistic Regression\n",
      "Results for Domain with Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.67      0.57      0.62         7\n",
      "    Education       1.00      0.80      0.89        10\n",
      "Entertainment       1.00      0.55      0.71        11\n",
      "      Finance       1.00      1.00      1.00        11\n",
      "        Media       0.44      0.88      0.58         8\n",
      "\n",
      "     accuracy                           0.77        47\n",
      "    macro avg       0.82      0.76      0.76        47\n",
      " weighted avg       0.85      0.77      0.78        47\n",
      "\n",
      "Best parameters found for Domain with Logistic Regression: {'C': 0.1}\n",
      "Training and evaluating Gradient Boosting Software...\n",
      "\n",
      "Evaluating Software with Gradient Boosting Software\n",
      "Results for Software with Gradient Boosting Software:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.55      0.67      0.60         9\n",
      "       cloud       1.00      0.94      0.97        18\n",
      "    frontend       0.43      0.43      0.43         7\n",
      "      gaming       0.86      0.67      0.75         9\n",
      "      mobile       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.77        47\n",
      "   macro avg       0.73      0.74      0.73        47\n",
      "weighted avg       0.78      0.77      0.77        47\n",
      "\n",
      "Best parameters found for Software with Gradient Boosting Software: {'max_depth': 5, 'n_estimators': 100}\n",
      "Training and evaluating Gradient Boosting Platform...\n",
      "\n",
      "Evaluating Platform with Gradient Boosting Platform\n",
      "Results for Platform with Gradient Boosting Platform:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.75      0.60      0.67         5\n",
      "           Android       1.00      0.57      0.73        14\n",
      "      Jio ChargeIT       0.33      1.00      0.50         1\n",
      "               Web       1.00      0.75      0.86        12\n",
      "               iOS       0.64      1.00      0.78         9\n",
      "streaming services       0.44      0.67      0.53         6\n",
      "\n",
      "          accuracy                           0.72        47\n",
      "         macro avg       0.70      0.76      0.68        47\n",
      "      weighted avg       0.82      0.72      0.73        47\n",
      "\n",
      "Best parameters found for Platform with Gradient Boosting Platform: {'max_depth': 3, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "sampled_data = pd.read_csv('final_data.csv')\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "\n",
    "# Function to apply SMOTE and check the number of samples\n",
    "def apply_smote(X_train, y_train):\n",
    "    if len(y_train.value_counts()) < 2:  # Check if there are fewer than 2 classes\n",
    "        print(\"Not enough classes for SMOTE.\")\n",
    "        return X_train, y_train  # Return original if not enough classes\n",
    "    return smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resample training set for each target variable\n",
    "X_train_domain, y_train_domain = apply_smote(X_train, y_train_domain)\n",
    "X_train_software, y_train_software = apply_smote(X_train, y_train_software)\n",
    "X_train_platform, y_train_platform = apply_smote(X_train, y_train_platform)\n",
    "\n",
    "# Define the model dictionary\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),  # For Domain\n",
    "    'Gradient Boosting Software': GradientBoostingClassifier(random_state=42),  # For Software\n",
    "    'Gradient Boosting Platform': GradientBoostingClassifier(random_state=42)  # For Platform\n",
    "}\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "    'Gradient Boosting Software': {'n_estimators': [50, 100], 'max_depth': [3, 5, 10]},\n",
    "    'Gradient Boosting Platform': {'n_estimators': [50, 100], 'max_depth': [3, 5, 10]}\n",
    "}\n",
    "\n",
    "# Loop through the models and perform training, tuning, and evaluation\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "\n",
    "    # Determine the target variable\n",
    "    if \"Logistic Regression\" in model_name:\n",
    "        target, X_train, y_train, y_test = 'Domain', X_train_domain, y_train_domain, y_test_domain\n",
    "    elif \"Software\" in model_name:\n",
    "        target, X_train, y_train, y_test = 'Software', X_train_software, y_train_software, y_test_software\n",
    "    else:  # For Gradient Boosting Platform\n",
    "        target, X_train, y_train, y_test = 'Platform', X_train_platform, y_train_platform, y_test_platform\n",
    "\n",
    "    print(f\"\\nEvaluating {target} with {model_name}\")\n",
    "\n",
    "    # Define GridSearchCV for each model\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=3, scoring='f1_macro')\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    print(f\"Results for {target} with {model_name}:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # Print best parameters for each model\n",
    "    print(f\"Best parameters found for {target} with {model_name}: {grid_search.best_params_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "c4cb47c9-0ffc-4842-afbf-bb17faee9705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Gradient Boosting for Domain...\n",
      "Results for Domain:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   E-commerce       0.67      0.86      0.75         7\n",
      "    Education       0.77      1.00      0.87        10\n",
      "Entertainment       1.00      0.55      0.71        11\n",
      "      Finance       1.00      0.91      0.95        11\n",
      "        Media       0.67      0.75      0.71         8\n",
      "\n",
      "     accuracy                           0.81        47\n",
      "    macro avg       0.82      0.81      0.80        47\n",
      " weighted avg       0.84      0.81      0.80        47\n",
      "\n",
      "Best parameters found for Domain: {'max_depth': 3, 'n_estimators': 100}\n",
      "Training and evaluating Gradient Boosting for Software...\n",
      "Results for Software:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     backend       0.55      0.67      0.60         9\n",
      "       cloud       1.00      0.94      0.97        18\n",
      "    frontend       0.43      0.43      0.43         7\n",
      "      gaming       0.86      0.67      0.75         9\n",
      "      mobile       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.77        47\n",
      "   macro avg       0.73      0.74      0.73        47\n",
      "weighted avg       0.78      0.77      0.77        47\n",
      "\n",
      "Best parameters found for Software: {'max_depth': 5, 'n_estimators': 100}\n",
      "Training and evaluating Gradient Boosting for Platform...\n",
      "Results for Platform:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "                         0.75      0.60      0.67         5\n",
      "           Android       1.00      0.57      0.73        14\n",
      "      Jio ChargeIT       0.33      1.00      0.50         1\n",
      "               Web       1.00      0.75      0.86        12\n",
      "               iOS       0.64      1.00      0.78         9\n",
      "streaming services       0.44      0.67      0.53         6\n",
      "\n",
      "          accuracy                           0.72        47\n",
      "         macro avg       0.70      0.76      0.68        47\n",
      "      weighted avg       0.82      0.72      0.73        47\n",
      "\n",
      "Best parameters found for Platform: {'max_depth': 3, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset\n",
    "sampled_data = pd.read_csv('final_data.csv')\n",
    "\n",
    "# Preprocessing: Fill NaN values and convert target columns to string\n",
    "sampled_data['combined_text'] = sampled_data['combined_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare target variables\n",
    "y_domain = sampled_data['Domain [Healthcare, Ecommerce, Finance, Education, Entertainment, Media]'].fillna('').astype(str)\n",
    "y_software = sampled_data['Software Application [frontent, backend, mobile, desktop, cloud, gaming]'].fillna('').astype(str)\n",
    "y_platform = sampled_data['Platform [AndroidTV,  ATV, H5TV, iOS, Android,Web, Jio STB, Jio Cinema, Jio ChargeIT, Celia, All Platforms, SDK, JioTV, Stage, Spike, Playstore, Fanverse, CTV, VOOT]'].fillna('').astype(str)\n",
    "\n",
    "# Preprocessing and feature extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sampled_data['combined_text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train_domain, y_test_domain = train_test_split(X, y_domain, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_software, y_test_software = train_test_split(X, y_software, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_platform, y_test_platform = train_test_split(X, y_platform, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "\n",
    "# Function to apply SMOTE and check the number of samples\n",
    "def apply_smote(X_train, y_train):\n",
    "    if len(y_train.value_counts()) < 2:  # Check if there are fewer than 2 classes\n",
    "        print(\"Not enough classes for SMOTE.\")\n",
    "        return X_train, y_train  # Return original if not enough classes\n",
    "    return smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resample training set for each target variable\n",
    "X_train_domain, y_train_domain = apply_smote(X_train, y_train_domain)\n",
    "X_train_software, y_train_software = apply_smote(X_train, y_train_software)\n",
    "X_train_platform, y_train_platform = apply_smote(X_train, y_train_platform)\n",
    "\n",
    "# Define the model and parameter grid for hyperparameter tuning\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "param_grid = {'n_estimators': [50, 100], 'max_depth': [3, 5, 10]}\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate(target_name, X_train, y_train, X_test, y_test):\n",
    "    print(f\"Training and evaluating Gradient Boosting for {target_name}...\")\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='f1_macro')\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    print(f\"Results for {target_name}:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    \n",
    "    # Print best parameters for each model\n",
    "    print(f\"Best parameters found for {target_name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Train and evaluate for all three targets\n",
    "train_and_evaluate('Domain', X_train_domain, y_train_domain, X_test, y_test_domain)\n",
    "train_and_evaluate('Software', X_train_software, y_train_software, X_test, y_test_software)\n",
    "train_and_evaluate('Platform', X_train_platform, y_train_platform, X_test, y_test_platform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3e493-4bc2-471c-ab78-bafa72d5bf09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
