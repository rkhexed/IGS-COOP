{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0591de-3b36-4284-8975-9482c3f032dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5664d-1c61-4525-bbe2-f1171ea10c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install trulens trulens-apps-langchain trulens-providers-litellm litellm langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb10415-1740-44a8-88ab-906678969e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import TruSession\n",
    "from trulens.apps.langchain import TruChain\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7dce4d-c44e-4681-9690-5cbdaccaa946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize Ollama with the correct class name\n",
    "ollama = OllamaLLM(base_url=\"http://localhost:11434\", model=\"llama2\")\n",
    "\n",
    "# Query the model\n",
    "response = ollama(\"Why is the sky blue?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eeccd0-3e22-4033-bb44-c2845de07138",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9cb36-4e1b-450c-ad3c-47c12252c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Ollama's local server URL (adjust as needed)\n",
    "ollama_url = \"http://localhost:11434/models\"\n",
    "\n",
    "# Send a GET request to fetch models\n",
    "response = requests.get(ollama_url)\n",
    "\n",
    "# Check the response status and print the available models\n",
    "if response.status_code == 200:\n",
    "    models = response.json()\n",
    "    print(\"Available models:\", models)\n",
    "else:\n",
    "    print(f\"Error fetching models: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a690150-6839-4917-823a-ef6960478d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from trulens_eval import Tru, TruChain\n",
    "from trulens_eval.keys import openai\n",
    "from trulens_eval.feedback import Feedback\n",
    "from trulens_eval.instruments import Instrument\n",
    "from trulens_eval.schema import FeedbackResult\n",
    "import numpy as np\n",
    "\n",
    "# Disable the OpenAI dependency warning\n",
    "openai.api_key = \"not_required\"\n",
    "\n",
    "# Initialize TruLens tracking\n",
    "tru = Tru()\n",
    "\n",
    "# Create a Hugging Face text generation pipeline\n",
    "generator = pipeline(\n",
    "    'text-generation', \n",
    "    model='gpt2', \n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Define custom feedback functions\n",
    "def coherence_feedback(output: str) -> float:\n",
    "    \"\"\"\n",
    "    Simple coherence check: Measure sentence variation \n",
    "    and complexity of generated text\n",
    "    \"\"\"\n",
    "    sentences = output.split('.')\n",
    "    if len(sentences) <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    if not sentence_lengths:\n",
    "        return 0.0\n",
    "    \n",
    "    length_variation = np.std(sentence_lengths)\n",
    "    avg_length = np.mean(sentence_lengths)\n",
    "    \n",
    "    # Normalize score between 0 and 1\n",
    "    coherence_score = min(1.0, max(0.0, 1 - (length_variation / avg_length)))\n",
    "    return coherence_score\n",
    "\n",
    "def toxicity_feedback(output: str) -> float:\n",
    "    \"\"\"\n",
    "    Basic toxicity estimation\n",
    "    Returns 1.0 if toxic, 0.0 if not toxic\n",
    "    \"\"\"\n",
    "    toxic_words = ['hate', 'stupid', 'idiot', 'terrible', 'worst']\n",
    "    output_lower = output.lower()\n",
    "    \n",
    "    toxicity_count = sum(1 for word in toxic_words if word in output_lower)\n",
    "    return min(1.0, toxicity_count * 0.2)\n",
    "\n",
    "# Create feedback functions\n",
    "coherence_fn = Feedback(coherence_feedback)\n",
    "toxicity_fn = Feedback(toxicity_feedback)\n",
    "\n",
    "# Wrapper function for LLM call\n",
    "def generate_text(prompt: str, max_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Hugging Face pipeline with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = generator(\n",
    "            prompt, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            do_sample=True\n",
    "        )\n",
    "        return result[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {e}\")\n",
    "        return \"Error in text generation\"\n",
    "\n",
    "# Instrument the text generation function\n",
    "instrumented_generate = Instrument(generate_text)\n",
    "\n",
    "# Create a TruChain to track the generation\n",
    "with tru.chain(\n",
    "    instrumented_generate, \n",
    "    feedbacks=[coherence_fn, toxicity_fn]\n",
    ") as chain:\n",
    "    # Example usage\n",
    "    prompt = \"Tell me a short story about adventure\"\n",
    "    output = chain(prompt)\n",
    "    print(\"Generated Text:\", output)\n",
    "\n",
    "# Record the record\n",
    "tru.run_dashboard()\n",
    "\n",
    "# Optional: Print out results\n",
    "tru.get_records_and_feedback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f71fc-eb12-40f1-b4c9-cb72883ba5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c28ac-7e33-41bc-b5e5-40b16f7e8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install trulens-eval transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81694675-8821-4a57-b5c1-08508742c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install trulens-eval transformers torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d05a9c-d46a-4da8-a564-0041ad27a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from trulens_eval import Tru, Feedback\n",
    "from trulens_eval.experimental.instrumentation import Instrument\n",
    "import numpy as np\n",
    "\n",
    "# Initialize TruLens tracking\n",
    "tru = Tru()\n",
    "\n",
    "# Create a Hugging Face text generation pipeline\n",
    "generator = pipeline(\n",
    "    'text-generation', \n",
    "    model='gpt2', \n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Define custom feedback functions\n",
    "def coherence_feedback(output: str) -> float:\n",
    "    \"\"\"\n",
    "    Simple coherence check: Measure sentence variation \n",
    "    and complexity of generated text\n",
    "    \"\"\"\n",
    "    sentences = output.split('.')\n",
    "    if len(sentences) <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    if not sentence_lengths:\n",
    "        return 0.0\n",
    "    \n",
    "    length_variation = np.std(sentence_lengths)\n",
    "    avg_length = np.mean(sentence_lengths)\n",
    "    \n",
    "    # Normalize score between 0 and 1\n",
    "    coherence_score = min(1.0, max(0.0, 1 - (length_variation / avg_length)))\n",
    "    return coherence_score\n",
    "\n",
    "def toxicity_feedback(output: str) -> float:\n",
    "    \"\"\"\n",
    "    Basic toxicity estimation\n",
    "    Returns 1.0 if toxic, 0.0 if not toxic\n",
    "    \"\"\"\n",
    "    toxic_words = ['hate', 'stupid', 'idiot', 'terrible', 'worst']\n",
    "    output_lower = output.lower()\n",
    "    \n",
    "    toxicity_count = sum(1 for word in toxic_words if word in output_lower)\n",
    "    return min(1.0, toxicity_count * 0.2)\n",
    "\n",
    "# Create feedback functions using Feedback class\n",
    "coherence_fn = Feedback(coherence_feedback)\n",
    "toxicity_fn = Feedback(toxicity_feedback)\n",
    "\n",
    "# Wrapper function for LLM call\n",
    "def generate_text(prompt: str, max_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Hugging Face pipeline with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = generator(\n",
    "            prompt, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            do_sample=True\n",
    "        )\n",
    "        return result[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {e}\")\n",
    "        return \"Error in text generation\"\n",
    "\n",
    "# Instrument the text generation function\n",
    "@Instrument\n",
    "def instrumented_generate(prompt: str) -> str:\n",
    "    return generate_text(prompt)\n",
    "\n",
    "# Configure TruLens logging directory\n",
    "tru_logging_dir = os.path.join(os.getcwd(), 'trulens_logs')\n",
    "os.makedirs(tru_logging_dir, exist_ok=True)\n",
    "tru.config.database_path = tru_logging_dir\n",
    "\n",
    "# Record the record with feedbacks\n",
    "def main():\n",
    "    with tru.chain(\n",
    "        instrumented_generate, \n",
    "        feedbacks=[coherence_fn, toxicity_fn]\n",
    "    ) as chain:\n",
    "        # Example usage\n",
    "        prompt = \"Tell me a short story about adventure\"\n",
    "        output = chain(prompt)\n",
    "        print(\"Generated Text:\", output)\n",
    "\n",
    "    # Generate and print records\n",
    "    records = tru.get_records_and_feedback()\n",
    "    print(\"\\nRecords and Feedback:\")\n",
    "    for record in records:\n",
    "        print(f\"Prompt: {record.prompt}\")\n",
    "        print(f\"Response: {record.response}\")\n",
    "        print(\"Feedback Scores:\")\n",
    "        for feedback in record.feedback:\n",
    "            print(f\"  {feedback.name}: {feedback.result}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Optional: Run dashboard\n",
    "    tru.run_dashboard()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0753acb-7f23-4d25-a77b-716c25445a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install trulens_eval.experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b078d2a-845e-48f3-bfb7-fd5fb4ec905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from trulens_eval import Tru, Feedback\n",
    "from trulens_eval.utils.containers import Context\n",
    "import numpy as np\n",
    "\n",
    "# Initialize TruLens tracking\n",
    "tru = Tru()\n",
    "\n",
    "# Create a Hugging Face text generation pipeline\n",
    "generator = pipeline(\n",
    "    'text-generation', \n",
    "    model='gpt2', \n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Define custom feedback functions\n",
    "def coherence_feedback(context: Context) -> float:\n",
    "    \"\"\"\n",
    "    Simple coherence check: Measure sentence variation \n",
    "    and complexity of generated text\n",
    "    \"\"\"\n",
    "    output = context.get_response()\n",
    "    sentences = output.split('.')\n",
    "    if len(sentences) <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    if not sentence_lengths:\n",
    "        return 0.0\n",
    "    \n",
    "    length_variation = np.std(sentence_lengths)\n",
    "    avg_length = np.mean(sentence_lengths)\n",
    "    \n",
    "    # Normalize score between 0 and 1\n",
    "    coherence_score = min(1.0, max(0.0, 1 - (length_variation / avg_length)))\n",
    "    return coherence_score\n",
    "\n",
    "def toxicity_feedback(context: Context) -> float:\n",
    "    \"\"\"\n",
    "    Basic toxicity estimation\n",
    "    Returns 1.0 if toxic, 0.0 if not toxic\n",
    "    \"\"\"\n",
    "    output = context.get_response()\n",
    "    toxic_words = ['hate', 'stupid', 'idiot', 'terrible', 'worst']\n",
    "    output_lower = output.lower()\n",
    "    \n",
    "    toxicity_count = sum(1 for word in toxic_words if word in output_lower)\n",
    "    return min(1.0, toxicity_count * 0.2)\n",
    "\n",
    "# Create feedback functions\n",
    "coherence_fn = Feedback(coherence_feedback)\n",
    "toxicity_fn = Feedback(toxicity_feedback)\n",
    "\n",
    "# Wrapper function for LLM call\n",
    "def generate_text(prompt: str, max_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Hugging Face pipeline with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = generator(\n",
    "            prompt, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            do_sample=True\n",
    "        )\n",
    "        return result[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {e}\")\n",
    "        return \"Error in text generation\"\n",
    "\n",
    "# Configure TruLens logging directory\n",
    "tru_logging_dir = os.path.join(os.getcwd(), 'trulens_logs')\n",
    "os.makedirs(tru_logging_dir, exist_ok=True)\n",
    "tru.config.database_path = tru_logging_dir\n",
    "\n",
    "def main():\n",
    "    # Record the record with feedbacks\n",
    "    with tru.track(\n",
    "        generate_text, \n",
    "        feedbacks=[coherence_fn, toxicity_fn]\n",
    "    ) as chain:\n",
    "        # Example usage\n",
    "        prompt = \"Tell me a short story about adventure\"\n",
    "        output = chain(prompt)\n",
    "        print(\"Generated Text:\", output)\n",
    "\n",
    "    # Generate and print records\n",
    "    records = tru.get_records_and_feedback()\n",
    "    print(\"\\nRecords and Feedback:\")\n",
    "    for record in records:\n",
    "        print(f\"Prompt: {record.get_input()}\")\n",
    "        print(f\"Response: {record.get_response()}\")\n",
    "        print(\"Feedback Scores:\")\n",
    "        for feedback in record.feedbacks:\n",
    "            print(f\"  {feedback.name}: {feedback.result}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Optional: Run dashboard\n",
    "    tru.run_dashboard()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953a070-6150-475b-8dab-afec4d6010a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install trulens-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc66760-a5dc-44c8-a3c3-a6b4c8d2db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from trulens_eval import Tru, TruChain\n",
    "from trulens_eval.feedback import Feedback, Groundedness\n",
    "from trulens_eval.feedback.provider.openai import OpenAI\n",
    "\n",
    "# Initialize TruLens and OpenAI feedback provider\n",
    "tru = Tru()\n",
    "openai = OpenAI()\n",
    "\n",
    "# Create a Hugging Face text generation pipeline\n",
    "generator = pipeline(\n",
    "    'text-generation', \n",
    "    model='gpt2', \n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "def generate_text(prompt: str, max_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Hugging Face pipeline with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = generator(\n",
    "            prompt, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            do_sample=True\n",
    "        )\n",
    "        return result[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Generation error: {e}\")\n",
    "        return \"Error in text generation\"\n",
    "\n",
    "def main():\n",
    "    # Configure logging directory\n",
    "    tru_logging_dir = os.path.join(os.getcwd(), 'trulens_logs')\n",
    "    os.makedirs(tru_logging_dir, exist_ok=True)\n",
    "    tru.config.database_path = tru_logging_dir\n",
    "\n",
    "    # Define feedback functions using OpenAI\n",
    "    # Groundedness check\n",
    "    grounded = Groundedness(groundedness_provider=openai)\n",
    "    groundedness_feedback = Feedback(grounded.groundedness_measure_with_cot_reasons).on_output()\n",
    "\n",
    "    # Relevance feedback\n",
    "    relevance_feedback = Feedback(openai.relevance).on_input_output()\n",
    "\n",
    "    # Quality feedback\n",
    "    quality_feedback = Feedback(openai.quality).on_output()\n",
    "\n",
    "    # Track the text generation with TruChain\n",
    "    chain = TruChain(\n",
    "        generate_text, \n",
    "        app_id = 'GPT2_Text_Generation_App',\n",
    "        feedbacks = [\n",
    "            groundedness_feedback, \n",
    "            relevance_feedback, \n",
    "            quality_feedback\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Example prompts\n",
    "    prompts = [\n",
    "        \"Tell me a short story about adventure\",\n",
    "        \"Describe a peaceful mountain landscape\",\n",
    "        \"Write a brief poem about friendship\"\n",
    "    ]\n",
    "\n",
    "    # Generate and track responses\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        # Use the chain to generate and record\n",
    "        response = chain.run(prompt)\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "    # Retrieve and display records\n",
    "    records = tru.get_records_and_feedback(app_ids=['GPT2_Text_Generation_App'])\n",
    "    print(\"\\n--- Feedback Records ---\")\n",
    "    for record in records:\n",
    "        print(f\"Prompt: {record.input}\")\n",
    "        print(f\"Response: {record.output}\")\n",
    "        print(\"Feedback:\")\n",
    "        for feedback in record.feedback:\n",
    "            print(f\"  {feedback.name}: {feedback.result}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Open the dashboard\n",
    "    tru.run_dashboard()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbcbbd-3ee2-4cf3-bdca-01addb2d2848",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install arize-phoenix transformers torch pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac1d83-2e20-4e38-a386-4085cc21d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import phoenix as px\n",
    "import openai\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration and setup\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Configure OpenAI (optional, for more advanced uses)\n",
    "# openai.api_key = 'your-api-key'\n",
    "\n",
    "def generate_text(prompt: str, max_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Hugging Face pipeline with robust error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use GPT-2 for text generation\n",
    "        generator = pipeline('text-generation', model='gpt2')\n",
    "        \n",
    "        # Generate text with safety checks\n",
    "        result = generator(\n",
    "            prompt, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        # Extract and clean generated text\n",
    "        generated_text = result[0]['generated_text'].strip()\n",
    "        return generated_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Text generation error: {e}\")\n",
    "        return f\"Generation error: {str(e)}\"\n",
    "\n",
    "def main():\n",
    "    # Initialize Phoenix session\n",
    "    session = px.launch_app()\n",
    "\n",
    "    # Prepare example prompts\n",
    "    prompts = [\n",
    "        \"Tell me a short story about adventure\",\n",
    "        \"Describe a peaceful mountain landscape\",\n",
    "        \"Write a brief poem about friendship\"\n",
    "    ]\n",
    "\n",
    "    # Lists to collect data for DataFrame\n",
    "    generations = []\n",
    "\n",
    "    # Generate texts and collect data\n",
    "    for prompt in prompts:\n",
    "        response = generate_text(prompt)\n",
    "        \n",
    "        # Collect generation data\n",
    "        generation_data = {\n",
    "            'prompt': prompt,\n",
    "            'response': response,\n",
    "            'response_length': len(response),\n",
    "            'word_count': len(response.split()),\n",
    "        }\n",
    "        generations.append(generation_data)\n",
    "\n",
    "    # Create DataFrame for tracking\n",
    "    generations_df = pd.DataFrame(generations)\n",
    "\n",
    "    # Basic custom evaluations\n",
    "    def calculate_complexity(text):\n",
    "        \"\"\"Calculate text complexity based on word length and sentence structure\"\"\"\n",
    "        words = text.split()\n",
    "        avg_word_length = np.mean([len(word) for word in words])\n",
    "        unique_word_ratio = len(set(words)) / len(words)\n",
    "        return avg_word_length * unique_word_ratio\n",
    "\n",
    "    generations_df['complexity_score'] = generations_df['response'].apply(calculate_complexity)\n",
    "\n",
    "    # Log DataFrame to Phoenix\n",
    "    px.log_dataframe(generations_df, 'text_generations')\n",
    "\n",
    "    # Print out generations for verification\n",
    "    print(\"\\nGeneration Results:\")\n",
    "    print(generations_df)\n",
    "\n",
    "    # Keep the Phoenix session open\n",
    "    session.wait()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d017ce-5b8b-4dbb-87d3-53e8c9ad4a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arize-phoenix in /opt/anaconda3/lib/python3.12/site-packages (7.1.1)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.46.2)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: aioitertools in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.7.1)\n",
      "Requirement already satisfied: aiosqlite in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.20.0)\n",
      "Requirement already satisfied: alembic<2,>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (1.14.0)\n",
      "Requirement already satisfied: arize-phoenix-evals>=0.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.17.5)\n",
      "Requirement already satisfied: arize-phoenix-otel>=0.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.6.1)\n",
      "Requirement already satisfied: authlib in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (1.3.2)\n",
      "Requirement already satisfied: cachetools in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (5.3.3)\n",
      "Requirement already satisfied: fastapi in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.115.5)\n",
      "Requirement already satisfied: grpc-interceptor in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.15.4)\n",
      "Requirement already satisfied: grpcio in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (1.66.1)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.27.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (3.1.4)\n",
      "Requirement already satisfied: openinference-instrumentation>=0.1.12 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.1.19)\n",
      "Requirement already satisfied: openinference-semantic-conventions>=0.1.12 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.1.12)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-proto>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-sdk in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.50b0)\n",
      "Requirement already satisfied: protobuf<6.0,>=3.20.2 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (5.29.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (5.9.0)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (18.0.0)\n",
      "Requirement already satisfied: pydantic!=2.0.*,<3,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (2.10.1)\n",
      "Requirement already satisfied: python-multipart in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.0.19)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (1.13.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (2.0.30)\n",
      "Requirement already satisfied: sqlean-py>=3.45.1 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (3.47.0)\n",
      "Requirement already satisfied: starlette in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.41.3)\n",
      "Requirement already satisfied: strawberry-graphql==0.253.1 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.253.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (4.12.2)\n",
      "Requirement already satisfied: uvicorn in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (0.32.1)\n",
      "Requirement already satisfied: websockets in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (14.1)\n",
      "Requirement already satisfied: wrapt>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from arize-phoenix) (1.17.0)\n",
      "Requirement already satisfied: graphql-core<3.4.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from strawberry-graphql==0.253.1->arize-phoenix) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from strawberry-graphql==0.253.1->arize-phoenix) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: Mako in /opt/anaconda3/lib/python3.12/site-packages (from alembic<2,>=1.3.0->arize-phoenix) (1.3.8)\n",
      "Requirement already satisfied: opentelemetry-api in /opt/anaconda3/lib/python3.12/site-packages (from openinference-instrumentation>=0.1.12->arize-phoenix) (1.29.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=2.0.*,<3,>=1.0->arize-phoenix) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=2.0.*,<3,>=1.0->arize-phoenix) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.7.0->strawberry-graphql==0.253.1->arize-phoenix) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (3.0.1)\n",
      "Requirement already satisfied: cryptography in /opt/anaconda3/lib/python3.12/site-packages (from authlib->arize-phoenix) (42.0.5)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from starlette->arize-phoenix) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx->arize-phoenix) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->arize-phoenix) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx->arize-phoenix) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->arize-phoenix) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->arize-phoenix) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->arize-phoenix) (2.1.3)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.29.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp->arize-phoenix) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.29.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp->arize-phoenix) (1.29.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.29.0->opentelemetry-exporter-otlp->arize-phoenix) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.29.0->opentelemetry-exporter-otlp->arize-phoenix) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.29.0->opentelemetry-exporter-otlp->arize-phoenix) (1.29.0)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /opt/anaconda3/lib/python3.12/site-packages (from opentelemetry-api->openinference-instrumentation>=0.1.12->arize-phoenix) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->arize-phoenix) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->arize-phoenix) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn->arize-phoenix) (8.1.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.12/site-packages (from cryptography->authlib->arize-phoenix) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography->authlib->arize-phoenix) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api->openinference-instrumentation>=0.1.12->arize-phoenix) (3.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arize-phoenix transformers torch pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fef6533-fe68-45a0-8e04-df291f58e3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation Results:\n",
      "                                   prompt  \\\n",
      "0   Tell me a short story about adventure   \n",
      "1  Describe a peaceful mountain landscape   \n",
      "2     Write a brief poem about friendship   \n",
      "\n",
      "                                            response  response_length  \\\n",
      "0  Tell me a short story about adventure. I just ...              487   \n",
      "1  Describe a peaceful mountain landscape, especi...              447   \n",
      "2  Write a brief poem about friendship without be...              478   \n",
      "\n",
      "   word_count  complexity_score  \n",
      "0          90          2.849877  \n",
      "1          81          3.226642  \n",
      "2          91          2.796764  \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'phoenix' has no attribute 'collect_llm_generations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 95\u001b[0m\n\u001b[1;32m     88\u001b[0m     px\u001b[38;5;241m.\u001b[39mcollect_llm_generations(\n\u001b[1;32m     89\u001b[0m         dataframe\u001b[38;5;241m=\u001b[39mgenerations_df, \n\u001b[1;32m     90\u001b[0m         prompt_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     91\u001b[0m         response_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[3], line 88\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(generations_df)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Use Phoenix's ingest method instead of log_dataframe\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m px\u001b[38;5;241m.\u001b[39mcollect_llm_generations(\n\u001b[1;32m     89\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mgenerations_df, \n\u001b[1;32m     90\u001b[0m     prompt_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     91\u001b[0m     response_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     92\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'phoenix' has no attribute 'collect_llm_generations'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import phoenix as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "def generate_text(prompt: str, max_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Hugging Face pipeline with GPU support\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use GPU if available\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        \n",
    "        # Generate text with explicit truncation\n",
    "        generator = pipeline(\n",
    "            'text-generation', \n",
    "            model='gpt2', \n",
    "            device=device,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        result = generator(\n",
    "            prompt, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        generated_text = result[0]['generated_text'].strip()\n",
    "        return generated_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Text generation error: {e}\")\n",
    "        return f\"Generation error: {str(e)}\"\n",
    "\n",
    "def calculate_complexity(text):\n",
    "    \"\"\"Calculate text complexity based on word length and sentence structure\"\"\"\n",
    "    try:\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        avg_word_length = np.mean([len(word) for word in words])\n",
    "        unique_word_ratio = len(set(words)) / len(words)\n",
    "        return avg_word_length * unique_word_ratio\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def main():\n",
    "    # Explicitly start Phoenix session\n",
    "    px.launch_app()\n",
    "\n",
    "    # Prepare example prompts\n",
    "    prompts = [\n",
    "        \"Tell me a short story about adventure\",\n",
    "        \"Describe a peaceful mountain landscape\",\n",
    "        \"Write a brief poem about friendship\"\n",
    "    ]\n",
    "\n",
    "    # Lists to collect data for DataFrame\n",
    "    generations = []\n",
    "\n",
    "    # Generate texts and collect data\n",
    "    for prompt in prompts:\n",
    "        response = generate_text(prompt)\n",
    "        \n",
    "        # Collect generation data\n",
    "        generation_data = {\n",
    "            'prompt': prompt,\n",
    "            'response': response,\n",
    "            'response_length': len(response),\n",
    "            'word_count': len(response.split()),\n",
    "            'complexity_score': calculate_complexity(response)\n",
    "        }\n",
    "        generations.append(generation_data)\n",
    "\n",
    "    # Create DataFrame for tracking\n",
    "    generations_df = pd.DataFrame(generations)\n",
    "\n",
    "    # Print out generations for verification\n",
    "    print(\"\\nGeneration Results:\")\n",
    "    print(generations_df)\n",
    "\n",
    "    # Use Phoenix's ingest method instead of log_dataframe\n",
    "    px.collect_llm_generations(\n",
    "        dataframe=generations_df, \n",
    "        prompt_column='prompt', \n",
    "        response_column='response'\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be17a02-2bea-4fe1-ab90-68bb41092a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation Results:\n",
      "                                   prompt  \\\n",
      "0   Tell me a short story about adventure   \n",
      "1  Describe a peaceful mountain landscape   \n",
      "2     Write a brief poem about friendship   \n",
      "\n",
      "                                            response  response_length  \\\n",
      "0  Tell me a short story about adventure (with a ...              422   \n",
      "1  Describe a peaceful mountain landscape in Indi...              527   \n",
      "2  Write a brief poem about friendship and an ide...               63   \n",
      "\n",
      "   word_count  complexity_score  \n",
      "0          84          3.065760  \n",
      "1          87          3.537984  \n",
      "2          12          4.333333  \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'phoenix' has no attribute 'Predictions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m     session\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 101\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[12], line 88\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(generations_df)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Use Phoenix's data tracking\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m predictions \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mPredictions(\n\u001b[1;32m     89\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mgenerations_df, \n\u001b[1;32m     90\u001b[0m     input_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     91\u001b[0m     output_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Explicitly collect the predictions\u001b[39;00m\n\u001b[1;32m     95\u001b[0m px\u001b[38;5;241m.\u001b[39msave(predictions)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'phoenix' has no attribute 'Predictions'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import phoenix as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "def generate_text(prompt: str, max_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Hugging Face pipeline with GPU support\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use GPU if available\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        \n",
    "        # Generate text with explicit truncation\n",
    "        generator = pipeline(\n",
    "            'text-generation', \n",
    "            model='gpt2', \n",
    "            device=device,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        result = generator(\n",
    "            prompt, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        generated_text = result[0]['generated_text'].strip()\n",
    "        return generated_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Text generation error: {e}\")\n",
    "        return f\"Generation error: {str(e)}\"\n",
    "\n",
    "def calculate_complexity(text):\n",
    "    \"\"\"Calculate text complexity based on word length and sentence structure\"\"\"\n",
    "    try:\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        \n",
    "        avg_word_length = np.mean([len(word) for word in words])\n",
    "        unique_word_ratio = len(set(words)) / len(words)\n",
    "        return avg_word_length * unique_word_ratio\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def main():\n",
    "    # Explicitly start Phoenix session\n",
    "    session = px.launch_app()\n",
    "\n",
    "    # Prepare example prompts\n",
    "    prompts = [\n",
    "        \"Tell me a short story about adventure\",\n",
    "        \"Describe a peaceful mountain landscape\",\n",
    "        \"Write a brief poem about friendship\"\n",
    "    ]\n",
    "\n",
    "    # Lists to collect data for DataFrame\n",
    "    generations = []\n",
    "\n",
    "    # Generate texts and collect data\n",
    "    for prompt in prompts:\n",
    "        response = generate_text(prompt)\n",
    "        \n",
    "        # Collect generation data\n",
    "        generation_data = {\n",
    "            'prompt': prompt,\n",
    "            'response': response,\n",
    "            'response_length': len(response),\n",
    "            'word_count': len(response.split()),\n",
    "            'complexity_score': calculate_complexity(response)\n",
    "        }\n",
    "        generations.append(generation_data)\n",
    "\n",
    "    # Create DataFrame for tracking\n",
    "    generations_df = pd.DataFrame(generations)\n",
    "\n",
    "    # Print out generations for verification\n",
    "    print(\"\\nGeneration Results:\")\n",
    "    print(generations_df)\n",
    "\n",
    "    # Use Phoenix's data tracking\n",
    "    predictions = px.Predictions(\n",
    "        dataframe=generations_df, \n",
    "        input_column='prompt', \n",
    "        output_column='response'\n",
    "    )\n",
    "\n",
    "    # Explicitly collect the predictions\n",
    "    px.save(predictions)\n",
    "\n",
    "    # Keep the session open\n",
    "    session.wait()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01e9d882-04b1-4f12-915e-2b2c39cf2e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m     session\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 68\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     response \u001b[38;5;241m=\u001b[39m generate_text(prompt)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Create a trace for each generation\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     trace \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mevals(\n\u001b[1;32m     54\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m         kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m         inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt},\n\u001b[1;32m     57\u001b[0m         outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: response}\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     59\u001b[0m     traces\u001b[38;5;241m.\u001b[39mappend(trace)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Log traces to Phoenix\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import phoenix as px\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "def generate_text(prompt: str, max_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using Hugging Face pipeline with GPU support\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use GPU if available\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        \n",
    "        # Generate text with explicit truncation\n",
    "        generator = pipeline(\n",
    "            'text-generation', \n",
    "            model='gpt2', \n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        result = generator(\n",
    "            prompt, \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        generated_text = result[0]['generated_text'].strip()\n",
    "        return generated_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Text generation error: {e}\")\n",
    "        return f\"Generation error: {str(e)}\"\n",
    "\n",
    "def main():\n",
    "    # Launch Phoenix application\n",
    "    session = px.launch_app()\n",
    "\n",
    "    # Create sample data for LLM traces\n",
    "    traces = []\n",
    "    prompts = [\n",
    "        \"Tell me a short story about adventure\",\n",
    "        \"Describe a peaceful mountain landscape\",\n",
    "        \"Write a brief poem about friendship\"\n",
    "    ]\n",
    "\n",
    "    # Generate traces\n",
    "    for prompt in prompts:\n",
    "        response = generate_text(prompt)\n",
    "        \n",
    "        # Create a trace for each generation\n",
    "        trace = px.evals(\n",
    "            name=\"text_generation\",\n",
    "            kind=\"LLM\",\n",
    "            inputs={\"prompt\": prompt},\n",
    "            outputs={\"response\": response}\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    # Log traces to Phoenix\n",
    "    px.log_traces(traces)\n",
    "\n",
    "    # Keep the session open\n",
    "    session.wait()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f01c5d87-9737-4dbb-ab11-625a4e703c85",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopentelemetry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexporter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01motlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrace_exporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OTLPSpanExporter\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopentelemetry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_tracer_provider\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01marize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpheonix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArizePhoenix\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Set up OpenTelemetry tracing\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_tracing\u001b[39m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'arize'"
     ]
    }
   ],
   "source": [
    "#pip install requests opentelemetry-sdk opentelemetry-exporter-otlp arize-phoenix\n",
    "import requests\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.trace import set_tracer_provider\n",
    "from arize.pheonix import ArizePhoenix\n",
    "\n",
    "# Set up OpenTelemetry tracing\n",
    "def setup_tracing():\n",
    "    tracer_provider = TracerProvider()\n",
    "    span_processor = BatchSpanProcessor(OTLPSpanExporter(endpoint=\"http://localhost:4318/v1/traces\"))\n",
    "    tracer_provider.add_span_processor(span_processor)\n",
    "    set_tracer_provider(tracer_provider)\n",
    "    return trace.get_tracer(__name__)\n",
    "\n",
    "# Function that queries the model\n",
    "\"\"\"\n",
    "def query_ollama(prompt, model=\"model_name\", base_url=\"localhost:xyz\"):\n",
    "    url = f\"{base_url}/api/models/{model}/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\"prompt\": prompt}\n",
    "    \n",
    "    response = requests.post(url, json=data, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    return response.json().get(\"generated_text\", \"\")\n",
    "\"\"\"\n",
    "\n",
    "# Trace a single request and log the interaction\n",
    "def trace_and_log_request(tracer, prompt, model=\"llama2\", base_url=\"http://localhost:11434\"):\n",
    "    with tracer.start_as_current_span(\"ollama-query\") as span:\n",
    "        span.set_attribute(\"prompt\", prompt)\n",
    "        span.set_attribute(\"model\", model)\n",
    "        \n",
    "        response = query_ollama(prompt, model, base_url)\n",
    "        \n",
    "        span.set_attribute(\"response\", response)\n",
    "        return response\n",
    "\n",
    "# Prepare dataset and evaluate in Phoenix\n",
    "def evaluate_in_phoenix(prompts, responses, dataset_path=\"evaluation.jsonl\"):\n",
    "    # Write the dataset in JSONL format\n",
    "    with open(dataset_path, \"w\") as f:\n",
    "        for prompt, response in zip(prompts, responses):\n",
    "            record = {\"prompt\": prompt, \"response\": response}\n",
    "            f.write(f\"{json.dumps(record)}\\n\")\n",
    "    \n",
    "    # Evaluate using Phoenix\n",
    "    phoenix = ArizePhoenix()\n",
    "    phoenix.load(dataset_path)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Setup tracing\n",
    "    tracer = setup_tracing()\n",
    "\n",
    "    # Example prompts to query the model\n",
    "    prompts = [\n",
    "        \"Why is the sky blue?\",\n",
    "        \"Explain quantum entanglement in simple terms.\",\n",
    "        \"What is the capital of France?\"\n",
    "    ]\n",
    "    \n",
    "    # Trace each request and collect responses\n",
    "    responses = []\n",
    "    for prompt in prompts:\n",
    "        try:\n",
    "            response = trace_and_log_request(tracer, prompt)\n",
    "            responses.append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying Ollama for prompt '{prompt}': {e}\")\n",
    "            responses.append(\"Error\")\n",
    "    \n",
    "    # Evaluate results in Phoenix\n",
    "    evaluate_in_phoenix(prompts, responses)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc85745-3b32-4828-af8b-2f166571244c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
