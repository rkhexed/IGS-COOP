{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c05f14-e0b3-497a-9870-526b8a78c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"base version\"\n",
    "from pymongo import MongoClient\n",
    "\n",
    "database_name = \"OrgqrS1HZ\"\n",
    "collection_name = \"userStories\"\n",
    "milvus_collection_name = \"temp\" \n",
    "mongodb_uri = \"mongodb://localhost:27018\"\n",
    "\n",
    "mongo_client = MongoClient(mongodb_uri)\n",
    "db = mongo_client[database_name]\n",
    "collection = db[collection_name]\n",
    "documents = collection.find()\n",
    "for document in documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea5b17b-408e-4200-9ebd-c5630f75a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"final user story version\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ddbed-8e1b-4d02-874d-e100c49151e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from setfit import SetFitModel, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv(\"RAG Evaluation(classifier).csv\", encoding=\"ISO-8859-1\")  # Replace with your CSV file name\n",
    "\n",
    "# Extract queries and labels\n",
    "queries = data[\"query\"].tolist()\n",
    "labels = data[\"binary\"].apply(lambda x: 1 if x == \"TRUE\" else 0).tolist()  # Convert 'True'/'False' to 1 and 0\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_queries, test_queries, train_labels, test_labels = train_test_split(queries, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load a pre-trained sentence transformer model for SetFit\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "\n",
    "# Train the SetFit model on the few-shot examples\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=list(zip(train_queries, train_labels)),\n",
    "    eval_dataset=list(zip(test_queries, test_labels)),\n",
    "    loss_class=\"CosineSimilarityLoss\",\n",
    "    metric=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Accuracy:\", metrics[\"accuracy\"])\n",
    "\n",
    "# Define a function to classify a new user query\n",
    "def classify_query(query):\n",
    "    prediction = model.predict([query])\n",
    "    label = \"True\" if prediction[0] == 1 else \"False\"\n",
    "    return label\n",
    "\n",
    "# Test the classifier on a new user query\n",
    "user_query = \"Provide me a software development user story\"\n",
    "label = classify_query(user_query)\n",
    "print(f\"The query is classified as: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea4c2b-2525-4f51-8175-4f2928749869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from setfit import SetFitModel, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv(\"RAG Evaluation(classifier).csv\", encoding=\"ISO-8859-1\")  # Replace with your CSV file name\n",
    "\n",
    "# Extract queries and labels\n",
    "queries = data[\"query\"].tolist()\n",
    "labels = data[\"binary\"].apply(lambda x: 1 if x == \"TRUE\" else 0).tolist()  # Convert 'True'/'False' to 1 and 0\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_queries, test_queries, train_labels, test_labels = train_test_split(queries, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert training and testing sets to Dataset objects\n",
    "train_dataset = Dataset.from_dict({\"text\": train_queries, \"label\": train_labels})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_queries, \"label\": test_labels})\n",
    "\n",
    "# Load a pre-trained sentence transformer model for SetFit\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "\n",
    "# Use Trainer instead of SetFitTrainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    samples_per_label=110,  # Adjust based on data size and balance\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Accuracy:\", metrics[\"accuracy\"])\n",
    "\n",
    "# Define a function to classify a new user query\n",
    "def classify_query(query):\n",
    "    prediction = model.predict([query])\n",
    "    label = \"True\" if prediction[0] == 1 else \"False\"\n",
    "    return label\n",
    "\n",
    "# Test the classifier on a new user query\n",
    "user_query = \"Provide me a software development user story\"\n",
    "label = classify_query(user_query)\n",
    "print(f\"The query is classified as: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a873e-94a1-4137-be66-aa8184802420",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d6eb2-0fd1-4714-9611-8a3f8d272a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages if not already installed\n",
    "# !pip install flair pandas\n",
    "\n",
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.models import TARSClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv(\"RAG Evaluation(classifier).csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Convert labels to binary (1 for \"TRUE\", 0 for \"FALSE\")\n",
    "data[\"binary\"] = data[\"binary\"].apply(lambda x: 1 if x == \"TRUE\" else 0)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"binary\"])\n",
    "\n",
    "# Initialize the TARS model for text classification\n",
    "tars = TARSClassifier.load(\"tars-base\")\n",
    "tars.add_and_switch_to_new_task(\"Tech Query Classification\", label_dictionary=[\"TRUE\", \"FALSE\"])\n",
    "\n",
    "# Training the TARS model on the few-shot examples\n",
    "for index, row in train_data.iterrows():\n",
    "    sentence = Sentence(row[\"query\"])\n",
    "    sentence.set_label(\"Tech Query Classification\", \"TRUE\" if row[\"binary\"] == 1 else \"FALSE\")\n",
    "    tars.predict(sentence)\n",
    "\n",
    "# Evaluate model on the test dataset\n",
    "correct, total = 0, len(test_data)\n",
    "for index, row in test_data.iterrows():\n",
    "    sentence = Sentence(row[\"query\"])\n",
    "    tars.predict(sentence)\n",
    "    predicted_label = sentence.labels[0].value\n",
    "    correct += int(predicted_label == (\"TRUE\" if row[\"binary\"] == 1 else \"FALSE\"))\n",
    "\n",
    "print(\"Accuracy on test data:\", correct / total)\n",
    "\n",
    "# Define a function to classify a new user query\n",
    "def classify_query(query):\n",
    "    sentence = Sentence(query)\n",
    "    tars.predict(sentence)\n",
    "    return sentence.labels[0].value\n",
    "\n",
    "# Test the classifier on a new user query\n",
    "user_query = \"Provide me a software development user story\"\n",
    "print(\"The query is classified as:\", classify_query(user_query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea264712-ff97-4c16-a580-dadc0db370ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tars import TARS  # Corrected import\n",
    "from tars.data import Corpus\n",
    "from tars.trainer import ModelTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your data\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv(\"RAG Evaluation(classifier).csv\", encoding=\"ISO-8859-1\")  # Replace with your CSV file name\n",
    "\n",
    "# Extract queries and labels\n",
    "queries = data[\"query\"].tolist()\n",
    "labels = data[\"binary\"].apply(lambda x: 1 if x == \"TRUE\" else 0).tolist()  # Convert 'True'/'False' to 1 and 0\n",
    "\n",
    "# Create a Dataset using HuggingFace Dataset\n",
    "dataset_dict = {\n",
    "    \"train\": {\"query\": queries[:int(0.8 * len(queries))], \"label\": labels[:int(0.8 * len(labels))]},\n",
    "    \"test\": {\"query\": queries[int(0.8 * len(queries)):], \"label\": labels[int(0.8 * len(labels)):]}\n",
    "}\n",
    "\n",
    "# Create Dataset object for TARS\n",
    "train_dataset = Dataset.from_dict(dataset_dict[\"train\"])\n",
    "test_dataset = Dataset.from_dict(dataset_dict[\"test\"])\n",
    "\n",
    "# Create the corpus\n",
    "corpus = Corpus(train=train_dataset, test=test_dataset)\n",
    "\n",
    "# Initialize the TARS model\n",
    "tars = TARS.load('tars-base')  # Updated to correctly load TARS model\n",
    "\n",
    "# Define the label dictionary and task name\n",
    "label_dict = {\"TRUE\": 0, \"FALSE\": 1}\n",
    "\n",
    "# Add a new label type and switch to the new task\n",
    "tars.add_and_switch_to_new_task(label_type=\"tech_classifier\", label_dictionary=label_dict, task_name=\"tech_classification\")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = ModelTrainer(tars, corpus)\n",
    "\n",
    "# Train the model with progress bar\n",
    "tqdm.write(\"Training the model...\")\n",
    "trainer.train(\n",
    "    base_path=\"tars_model\",\n",
    "    learning_rate=3e-5,\n",
    "    mini_batch_size=8,\n",
    "    max_epochs=3\n",
    ")\n",
    "\n",
    "# Evaluate the model after training\n",
    "results = trainer.evaluate_model(corpus.test)\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "# Function to classify new queries\n",
    "def classify_query(query):\n",
    "    prediction = tars.predict([query])\n",
    "    label = \"TRUE\" if prediction[0] == 0 else \"FALSE\"\n",
    "    return label\n",
    "\n",
    "# Test the classifier on a new user query\n",
    "user_query = \"Provide me a software development user story\"\n",
    "label = classify_query(user_query)\n",
    "print(f\"The query is classified as: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f302c-b295-44dd-97ca-63522ab7a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load and preprocess your CSV data\n",
    "data = pd.read_csv(\"RAG Evaluation(classifier).csv\", encoding=\"ISO-8859-1\")  # Path to your CSV\n",
    "\n",
    "# Convert 'TRUE'/'FALSE' to 1/0\n",
    "data['label'] = data['binary'].apply(lambda x: 1 if x == \"TRUE\" else 0)\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data[['query', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_data[['query', 'label']])\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize datasets with padding\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"query\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Determine device (MPS or CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Move model to selected device\n",
    "model.to(device)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Training with progress bar\n",
    "print(\"Starting training...\")\n",
    "for _ in tqdm(range(int(training_args.num_train_epochs))):\n",
    "    trainer.train()\n",
    "\n",
    "# Evaluation\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", metrics)\n",
    "\n",
    "# Classification function for new queries\n",
    "def classify_query(query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "    inputs = inputs.to(device)  # Move inputs to the selected device\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    return \"TRUE\" if predicted_class_id == 1 else \"FALSE\"\n",
    "\n",
    "# Test the classifier\n",
    "user_query = \"Provide me a software development user story\"\n",
    "label = classify_query(user_query)\n",
    "print(f\"The query is classified as: {label}\")\n",
    "\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(f\"Accuracy: {eval_metrics['eval_accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e30381-d541-4650-a711-210203aa9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Step 1: Load your CSV data\n",
    "# Replace 'your_file.csv' with the actual path to your CSV\n",
    "data = pd.read_csv(\"RAG Evaluation(classifier).csv\", encoding=\"ISO-8859-1\")  # Replace with your CSV file name\n",
    "\n",
    "# Step 2: Prepare your dataset\n",
    "# Assuming your CSV has 'query' as the input text and 'binary' as the label (TRUE/FALSE)\n",
    "data[\"label\"] = data[\"binary\"].apply(lambda x: 1 if x == \"TRUE\" else 0)  # Convert 'TRUE'/'FALSE' to 1/0\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "dataset = Dataset.from_pandas(data[['query', 'label']])\n",
    "\n",
    "# Step 3: Tokenizer\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"query\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 4: Load the pre-trained model with the correct number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Step 5: Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,  # Load the best model based on evaluation results\n",
    ")\n",
    "\n",
    "# Step 6: Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets,  # Using the same data for evaluation\n",
    "    compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(axis=-1) == p.label_ids).mean()},\n",
    ")\n",
    "\n",
    "# Step 7: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results (including accuracy)\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Step 9: Inference (Classifying New Queries)\n",
    "def classify_query(query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    return \"TRUE\" if prediction == 1 else \"FALSE\"\n",
    "\n",
    "# Example: Test with a new query\n",
    "query = \"Provide a software development user story\"\n",
    "print(f\"The query is classified as: {classify_query(query)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15003216-f077-4312-a2db-2b2966a97fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load your own CSV file with queries and binary labels\n",
    "data = pd.read_csv(\"RAG Evaluation(classifier).csv\", encoding=\"ISO-8859-1\")  # Replace with your CSV file\n",
    "\n",
    "# Assuming your CSV contains columns 'query' and 'binary' where 'binary' is either \"TRUE\" or \"FALSE\"\n",
    "queries = data[\"query\"].tolist()\n",
    "labels = data[\"binary\"].apply(lambda x: \"TRUE\" if x == \"TRUE\" else \"FALSE\").tolist()\n",
    "\n",
    "# You can split your data into training and testing sets (you can also change the test_size as per your need)\n",
    "train_queries, test_queries, train_labels, test_labels = train_test_split(queries, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the zero-shot classification pipeline with a pre-trained model\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define the labels you're interested in\n",
    "candidate_labels = [\"TRUE\", \"FALSE\"]\n",
    "\n",
    "# Run zero-shot classification on the test set\n",
    "predictions = []\n",
    "for query in test_queries:\n",
    "    result = zero_shot_classifier(query, candidate_labels)\n",
    "    predictions.append(result[\"labels\"][0])  # Get the top label as the prediction\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(test_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ec10c-3ab6-46dc-9e50-704aa2580a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"binary\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d1a2e-4b57-401c-bc07-4706e532d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, BertTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert data to HuggingFace format\n",
    "data_dict = {\n",
    "    \"text\": train_queries,\n",
    "    \"label\": train_labels\n",
    "}\n",
    "train_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "data_dict = {\n",
    "    \"text\": test_queries,\n",
    "    \"label\": test_labels\n",
    "}\n",
    "test_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Load a pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize the datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set up the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    evaluation_strategy=\"epoch\",     # evaluate every epoch\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the model\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,           # evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2144b33-a0b4-493f-a63b-adfd231fb4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Load your CSV file\n",
    "data = pd.read_csv(\"RAG Evaluation(classifier).csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Step 2: Inspect the first few rows of your data (optional)\n",
    "print(data.head())\n",
    "\n",
    "# Step 3: Clean up the 'binary' column: map 'TRUE' to 1 and 'FALSE' to 0\n",
    "data['binary'] = data['binary'].apply(lambda x: 1 if x == \"TRUE\" else 0)\n",
    "\n",
    "# Step 4: Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "train_queries, test_queries, train_labels, test_labels = train_test_split(\n",
    "    data['query'], data['binary'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 5: Load the zero-shot classification pipeline from Hugging Face\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Step 6: Define the candidate labels for classification (these are your classes)\n",
    "candidate_labels = [\"TRUE\", \"FALSE\"]\n",
    "\n",
    "# Step 7: Function to classify a query using the zero-shot classifier\n",
    "def classify_query(query):\n",
    "    result = zero_shot_classifier(query, candidate_labels)\n",
    "    return result['labels'][0], result['scores'][0]\n",
    "\n",
    "# Step 8: Classify the queries in the test set and collect the predictions\n",
    "predictions = [classify_query(query)[0] for query in test_queries]\n",
    "\n",
    "# Step 9: Print the first 10 predictions (optional)\n",
    "print(predictions[:10])\n",
    "\n",
    "# Step 10: Evaluate the model using scikit-learn's classification_report\n",
    "print(\"Evaluation Results:\")\n",
    "print(classification_report(test_labels, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f51cc2-9413-4c24-ac52-d0cfa9af625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install setfit sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f960a-9fc8-4f89-bdaf-a48646c729cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from setfit import SetFitModel, Trainer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Load your dataset (replace 'your_data.csv' with the actual path to your CSV)\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Step 2: Ensure the 'binary' column has 0 (FALSE) and 1 (TRUE) values\n",
    "data['binary'] = data['binary'].apply(lambda x: 1 if x == 'TRUE' else 0)\n",
    "\n",
    "# Step 3: Split your dataset into train and test (using stratified split for balanced classes)\n",
    "train_queries, test_queries, train_labels, test_labels = train_test_split(\n",
    "    data['query'], data['binary'], test_size=0.2, stratify=data['binary']\n",
    ")\n",
    "\n",
    "# Step 4: Load the pre-trained sentence transformer model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 5: Initialize the SetFit model with a pre-trained transformer model\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 6: Prepare the training dataset (list of tuples of query and label)\n",
    "train_dataset = list(zip(train_queries, train_labels))\n",
    "test_dataset = list(zip(test_queries, test_labels))\n",
    "\n",
    "# Step 7: Define and train the model using the Trainer class\n",
    "trainer = Trainer(model=model, train_dataset=train_dataset, eval_dataset=test_dataset)\n",
    "\n",
    "# Train the model (this will use few-shot learning for the task)\n",
    "trainer.train()\n",
    "\n",
    "# Step 8: Evaluate the model on the test set\n",
    "y_pred = trainer.predict(test_queries)\n",
    "\n",
    "# Step 9: Output the classification report (accuracy, precision, recall, etc.)\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8ee384f-b5d0-4d28-9b84-9916d3456d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.73      0.85        45\n",
      "        True       0.76      1.00      0.87        39\n",
      "\n",
      "    accuracy                           0.86        84\n",
      "   macro avg       0.88      0.87      0.86        84\n",
      "weighted avg       0.89      0.86      0.86        84\n",
      "\n",
      "Prediction for the input query: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('RAG Evaluation(classifier).csv', encoding=\"ISO-8859-1\")\n",
    "X = df['query']\n",
    "y = df['binary']\n",
    "vectorizer = TfidfVectorizer(max_features=420)\n",
    "X_embeddings = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "def predict_query(query_text, model, vectorizer):\n",
    "    query_embedding = vectorizer.transform([query_text])\n",
    "\n",
    "    prediction = model.predict(query_embedding)\n",
    "    \n",
    "    return \"True\" if prediction[0] == 1 else \"False\"\n",
    "\n",
    "# Example\n",
    "new_query = \"Give me all the stories?\"\n",
    "print(\"Prediction for the input query:\", predict_query(new_query, model, vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "087cb0c7-a51e-487a-9a71-fa8b841de05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the input query: True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b39238ce-79df-4f66-b36f-73b51c69004e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: LogisticRegression\n",
      "Accuracy: 0.8571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.73      0.85        45\n",
      "        True       0.76      1.00      0.87        39\n",
      "\n",
      "    accuracy                           0.86        84\n",
      "   macro avg       0.88      0.87      0.86        84\n",
      "weighted avg       0.89      0.86      0.86        84\n",
      "\n",
      "\n",
      "Model: DecisionTreeClassifier\n",
      "Accuracy: 0.8571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.78      0.85        45\n",
      "        True       0.79      0.95      0.86        39\n",
      "\n",
      "    accuracy                           0.86        84\n",
      "   macro avg       0.87      0.86      0.86        84\n",
      "weighted avg       0.87      0.86      0.86        84\n",
      "\n",
      "\n",
      "Model: SVC\n",
      "Accuracy: 0.9523809523809523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.91      0.95        45\n",
      "        True       0.91      1.00      0.95        39\n",
      "\n",
      "    accuracy                           0.95        84\n",
      "   macro avg       0.95      0.96      0.95        84\n",
      "weighted avg       0.96      0.95      0.95        84\n",
      "\n",
      "\n",
      "Model: MultinomialNB\n",
      "Accuracy: 0.9404761904761905\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.89      0.94        45\n",
      "        True       0.89      1.00      0.94        39\n",
      "\n",
      "    accuracy                           0.94        84\n",
      "   macro avg       0.94      0.94      0.94        84\n",
      "weighted avg       0.95      0.94      0.94        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('RAG Evaluation(classifier).csv', encoding=\"ISO-8859-1\")\n",
    "X = df['query']\n",
    "y = df['binary']\n",
    "vectorizer = TfidfVectorizer(max_features=420)\n",
    "X_embeddings = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define function to train and evaluate model\n",
    "def evaluate_model(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\nModel: {model.__class__.__name__}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Initialize and evaluate models\n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC(),\n",
    "    MultinomialNB()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    evaluate_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2f299d7-b12e-426b-a1ce-b833c9346c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8809523809523809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.76      0.85        76\n",
      "        True       0.83      0.98      0.90        92\n",
      "\n",
      "    accuracy                           0.88       168\n",
      "   macro avg       0.90      0.87      0.88       168\n",
      "weighted avg       0.89      0.88      0.88       168\n",
      "\n",
      "Prediction for the input query: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "df = pd.read_csv('RAG Evaluation(classifier).csv', encoding=\"ISO-8859-1\")\n",
    "X = df['query']\n",
    "y = df['binary']\n",
    "vectorizer = TfidfVectorizer(max_features=420)\n",
    "X_embeddings = vectorizer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.4, random_state=42)\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "def predict_query(query_text, model, vectorizer):\n",
    "    query_embedding = vectorizer.transform([query_text])\n",
    "\n",
    "    prediction = model.predict(query_embedding)\n",
    "    \n",
    "    return \"True\" if prediction[0] == 1 else \"False\"\n",
    "\n",
    "# Example\n",
    "new_query = \"Give me all the stories?\"\n",
    "print(\"Prediction for the input query:\", predict_query(new_query, model, vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc92590f-edc7-4b77-a1f7-68c41fbc382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the input query: False\n"
     ]
    }
   ],
   "source": [
    "new_query = \"\"\n",
    "print(\"Prediction for the input query:\", predict_query(new_query, model, vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d57b3-0cfb-4329-a005-9d0c84209b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
