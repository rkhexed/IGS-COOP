{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d3be1ad-63a0-49f3-8d3e-214c2d87f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1 - ROUGE Scores: {'rouge1': 0.4437689969604863, 'rouge2': 0.3914373088685015, 'rougeL': 0.425531914893617, 'rougeLsum': 0.43161094224924007}\n",
      "Row 2 - ROUGE Scores: {'rouge1': 0.4142857142857143, 'rouge2': 0.11594202898550725, 'rougeL': 0.2714285714285714, 'rougeLsum': 0.2714285714285714}\n",
      "Row 3 - ROUGE Scores: {'rouge1': 0.11320754716981134, 'rouge2': 0.0, 'rougeL': 0.11320754716981134, 'rougeLsum': 0.11320754716981134}\n",
      "Row 4 - ROUGE Scores: {'rouge1': 0.37681159420289856, 'rouge2': 0.11707317073170731, 'rougeL': 0.2318840579710145, 'rougeLsum': 0.24154589371980678}\n",
      "Row 5 - ROUGE Scores: {'rouge1': 0.20833333333333331, 'rouge2': 0.1276595744680851, 'rougeL': 0.20833333333333331, 'rougeLsum': 0.20833333333333331}\n",
      "Row 6 - ROUGE Scores: {'rouge1': 0.7391304347826089, 'rouge2': 0.6323529411764706, 'rougeL': 0.6956521739130435, 'rougeLsum': 0.6376811594202898}\n",
      "Row 7 - ROUGE Scores: {'rouge1': 0.4301075268817204, 'rouge2': 0.3736263736263736, 'rougeL': 0.4086021505376344, 'rougeLsum': 0.4086021505376344}\n",
      "Row 8 - ROUGE Scores: {'rouge1': 0.25136612021857924, 'rouge2': 0.09944751381215469, 'rougeL': 0.20765027322404372, 'rougeLsum': 0.19672131147540983}\n",
      "Row 9 - ROUGE Scores: {'rouge1': 0.02816901408450704, 'rouge2': 0.0, 'rougeL': 0.02816901408450704, 'rougeLsum': 0.02816901408450704}\n",
      "Row 10 - ROUGE Scores: {'rouge1': 0.1253731343283582, 'rouge2': 0.06606606606606608, 'rougeL': 0.10746268656716418, 'rougeLsum': 0.11940298507462686}\n",
      "Row 11 - ROUGE Scores: {'rouge1': 0.4810126582278481, 'rouge2': 0.33766233766233766, 'rougeL': 0.379746835443038, 'rougeLsum': 0.379746835443038}\n",
      "Row 12 - ROUGE Scores: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n",
      "Row 13 - ROUGE Scores: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n",
      "Row 14 - ROUGE Scores: {'rouge1': 0.2183406113537118, 'rouge2': 0.026431718061674013, 'rougeL': 0.20087336244541484, 'rougeLsum': 0.1746724890829694}\n",
      "Row 15 - ROUGE Scores: {'rouge1': 0.08247422680412371, 'rouge2': 0.021052631578947368, 'rougeL': 0.08247422680412371, 'rougeLsum': 0.08247422680412371}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: nan,\nInput references: nan",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate ROUGE for each row and print results\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 12\u001b[0m     result \u001b[38;5;241m=\u001b[39m rouge\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39m[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Response V3\u001b[39m\u001b[38;5;124m'\u001b[39m]], references\u001b[38;5;241m=\u001b[39m[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequired Answer\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - ROUGE Scores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/evaluate/module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/evaluate/module.py:514\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m batch \u001b[38;5;241m=\u001b[39m {input_name: batch[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_batch(batch)\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/evaluate/module.py:596\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    595\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_example(example)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/evaluate/module.py:616\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_example\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m    609\u001b[0m feature_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature option \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures)])\n\u001b[1;32m    610\u001b[0m error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature_strings\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    615\u001b[0m )\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: nan,\nInput references: nan"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('RAG Evaluation(Dataset).csv')\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Calculate ROUGE for each row and print results\n",
    "for idx, row in df.iterrows():\n",
    "    result = rouge.compute(predictions=[row['Model Response V3']], references=[row['Required Answer']])\n",
    "    print(f\"Row {idx + 1} - ROUGE Scores: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7513fab-791f-480b-9658-c4b96139aeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Question', 'Required Answer', 'Model Answer', 'Model Answer V2',\n",
       "       'Retrieval Accuracy Score', 'Model Response Score', 'Rouge 1 ',\n",
       "       'Rouge L', 'Rouge 1 F1-Score', 'Difficulty Level',\n",
       "       'Retrieval Accuracy Score.1', 'Model Response Score.1', 'Rouge 1 .1',\n",
       "       'Rouge L.1', 'Model Response V3', 'Retrieval Accuracy Score.2',\n",
       "       'Model Response Score.2', 'Rouge 1 .2', 'Rouge L.2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44532ca4-c090-427b-beb6-ab0f92ce9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('RAG Evaluation(Dataset).csv')\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Calculate ROUGE-1 F1 score for each row and print results\n",
    "for idx, row in df.iterrows():\n",
    "    result = rouge.compute(predictions=[row['Model Answer V2']], references=[row['Required Answer']])\n",
    "    rouge_1_f1 = result[\"rouge1_fmeasure\"]\n",
    "    print(f\"Row {idx + 1} - ROUGE-1 F1 Score: {rouge_1_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b768290-6950-4d16-9d94-513a062bd808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('RAG Evaluation(Dataset).csv')\n",
    "\n",
    "# Filter out rows with NaN or empty values in relevant columns\n",
    "df = df.dropna(subset=['Model Answer V2', 'Required Answer'])\n",
    "df = df[(df['Model Answer V2'].str.strip() != '') & (df['Required Answer'].str.strip() != '')]\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Calculate ROUGE-1 F1 score for each row and print results\n",
    "for idx, row in df.iterrows():\n",
    "    result = rouge.compute(predictions=[row['Model Answer V2']], references=[row['Required Answer']])\n",
    "    rouge_1_f1 = result[\"rouge1\"]\n",
    "    print(f\"Row {idx + 1} - ROUGE-1 F1 Score: {rouge_1_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a314a3e-cd8b-4186-ab80-0b07f86f1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80113d6-c5e2-40c9-ab02-2758fb03e00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156e047-f422-4eef-a469-4f6d28c3c172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
